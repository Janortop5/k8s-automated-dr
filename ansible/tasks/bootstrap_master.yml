---
- name: Check if control plane is initialized
  ansible.builtin.stat:
    path: /etc/kubernetes/manifests/kube-apiserver.yaml
  register: cp_manifest

- name: Stop kubelet so it frees up ports
  become: true
  ansible.builtin.systemd:
    name: kubelet
    state: stopped
  when: cp_manifest.stat.exists

- name: Kill any stray kubeadm processes
  become: true
  ansible.builtin.shell:
     cmd: |
       # make sure kubelet is dead so ports 6443,10250,etc. are free
       systemctl stop kubelet
       # kill any half-baked kubeadm runs
       pkill kubeadm || true
  ignore_errors: true
  when: cp_manifest.stat.exists

- name: Reset any partial kubeadm state
  become: true
  ansible.builtin.shell: |
    kubeadm reset -f
    rm -rf /etc/kubernetes/manifests \
           /etc/kubernetes/pki \
           /etc/kubernetes/admin.conf \
           /var/lib/etcd
    crictl --runtime-endpoint unix:///run/containerd/containerd.sock rm --force $(crictl ps -aq) || true
    rm -rf /var/lib/kubelet/*
  ignore_errors: true
  when: cp_manifest.stat.exists

- name: Initialize Kubernetes control plane (no taint)
  become: true
  ansible.builtin.command: >
    kubeadm init
      --apiserver-advertise-address={{ ansible_default_ipv4.address }}
      --apiserver-cert-extra-sans={{ master_public_ip }}
      --skip-phases=mark-control-plane
  args:
    creates: /etc/kubernetes/manifests/kube-apiserver.yaml
  register: kubeadm_init

- name: Generate worker join command
  become: true
  ansible.builtin.command: kubeadm token create --print-join-command
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
  register: kubeadm_join

- name: Save join command for workers
  set_fact:
    join_command: "{{ kubeadm_join.stdout }}"

- name: Expose join_command to all worker-node hosts
  add_host:
    name: "{{ item }}"
    groups: worker-node
    join_command: "{{ join_command }}"
  loop: "{{ groups['worker-node'] }}"
  run_once: true

- name: Ensure kubeconfig directory exists
  file:
    path: "{{ ansible_env.HOME }}/.kube"
    state: directory
    mode: '0755'

- name: Copy admin.conf into user kubeconfig
  copy:
    src: /etc/kubernetes/admin.conf
    dest: "{{ ansible_env.HOME }}/.kube/config"
    owner: "{{ ansible_user }}"
    group: "{{ ansible_user }}"
    mode: '0644'
    remote_src: true

- name: Install Calico network plugin
  command: >
    kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml
  environment:
    KUBECONFIG: "{{ ansible_env.HOME }}/.kube/config"

- name: Slurp the admin kubeconfig from the master
  when: "'master-node' in group_names"
  ansible.builtin.slurp:
    src: /etc/kubernetes/admin.conf
  register: admin_conf_slurp
  run_once: true

- name: Ensure .kube dir in non-root user home
  ansible.builtin.file:
    path: "/home/{{ remote_user }}/.kube"
    state: directory
    mode: '0755'
    owner: "{{ remote_user }}"
    group: "{{ remote_user }}"
  become: true

- name: Deploy admin kubeconfig into non-root user ~/.kube/config
  ansible.builtin.copy:
    content: "{{ hostvars[ groups['master-node'][0] ].admin_conf_slurp.content | b64decode }}"
    dest: "/home/{{ remote_user }}/.kube/config"
    mode: '0644'
    owner: "{{ remote_user }}"
    group: "{{ remote_user }}"
  become: true
  when: admin_conf_slurp is defined
  no_log: true

- name: Install Local Path Provisioner & mark it default
  shell: |
    kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/master/deploy/local-path-storage.yaml
    kubectl patch storageclass local-path \
      -p '{"metadata":{"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'
  ignore_errors: true