{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T08:52:24.521652Z",
     "iopub.status.busy": "2025-06-05T08:52:24.521295Z",
     "iopub.status.idle": "2025-06-05T08:52:31.818886Z",
     "shell.execute_reply": "2025-06-05T08:52:31.817898Z",
     "shell.execute_reply.started": "2025-06-05T08:52:24.521609Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cu124\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "# Kubernetes LSTM Disaster Recovery System\n",
    "# Predicting CPU and Memory Usage for Pod-level Monitoring\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from datetime import datetime, timedelta\n",
    "import requests\n",
    "import json\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 1. DATA LOADING AND PREPROCESSING\n",
    "# =============================================================================\n",
    "\n",
    "class DataProcessor:\n",
    "    def __init__(self, csv_path):\n",
    "        self.csv_path = csv_path\n",
    "        self.scaler_features = StandardScaler()\n",
    "        self.scaler_targets = MinMaxScaler()\n",
    "        \n",
    "    def load_and_preprocess(self):\n",
    "        \"\"\"Load CSV and perform initial preprocessing\"\"\"\n",
    "        print(\"Loading data from CSV...\")\n",
    "        df = pd.read_csv(self.csv_path)\n",
    "        \n",
    "        # Convert timestamp to datetime\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'], format='%d/%m/%Y %H:%M') \n",
    "('%Y-%m-%d %H:%M')\n",
    " # pd.to_datetime(df['timestamp'])\n",
    "        \n",
    "        # Derive pod-level CPU and memory percentages\n",
    "        print(\"Deriving pod-level CPU and memory percentages...\")\n",
    "        df['pod_cpu_percentage'] = df['cpu_allocation_efficiency'] * df['node_cpu_usage']\n",
    "        df['pod_memory_percentage'] = df['memory_allocation_efficiency'] * df['node_memory_usage']\n",
    "        \n",
    "        # Clip values to reasonable ranges (0-100% for percentages)\n",
    "        df['pod_cpu_percentage'] = np.clip(df['pod_cpu_percentage'], 0, 100)\n",
    "        df['pod_memory_percentage'] = np.clip(df['pod_memory_percentage'], 0, 100)\n",
    "        \n",
    "        print(f\"Data shape: {df.shape}\")\n",
    "        print(f\"Unique pods: {df['pod_name'].nunique()}\")\n",
    "        print(f\"Date range: {df['timestamp'].min()} to {df['timestamp'].max()}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def resample_to_5min(self, df):\n",
    "        \"\"\"Resample 1-minute data to 5-minute intervals\"\"\"\n",
    "        print(\"Resampling data from 1-minute to 5-minute intervals...\")\n",
    "\n",
    "        new_df = df.copy()\n",
    "        resampled_dfs = []\n",
    "            \n",
    "        # Resample numeric columns\n",
    "        numeric_cols = ['cpu_allocation_efficiency', 'memory_allocation_efficiency', \n",
    "                      'disk_io', 'network_latency', 'node_temperature', \n",
    "                      'node_cpu_usage', 'node_memory_usage', 'pod_lifetime_seconds',\n",
    "                      'pod_cpu_percentage', 'pod_memory_percentage']\n",
    "        \n",
    "        resampled = new_df[numeric_cols].resample('5T').mean()\n",
    "        resampled['event_type'] = pod_df['event_type'].resample('5T').first()\n",
    "\n",
    "        resampled.dropna(inplace = True)\n",
    "        return resampled\n",
    "\n",
    "    def prepare_sequences(self, df, sequence_length=24, prediction_horizon=12):\n",
    "        \"\"\"\n",
    "        Prepare sequences for LSTM training\n",
    "        sequence_length: Number of past time steps to use (24 = 2 hours of 5-min data)\n",
    "        prediction_horizon: Number of future steps to predict (12 = 1 hour)\n",
    "        \"\"\"\n",
    "        print(f\"Preparing sequences (seq_len={sequence_length}, pred_horizon={prediction_horizon})...\")\n",
    "\n",
    "        df_copy = df.copy()\n",
    "        # Feature columns (excluding targets and identifiers)\n",
    "        feature_cols = ['cpu_allocation_efficiency', 'memory_allocation_efficiency', \n",
    "                       'disk_io', 'network_latency', 'node_temperature', \n",
    "                       'node_cpu_usage', 'node_memory_usage', 'pod_lifetime_seconds']\n",
    "        \n",
    "        # Target columns\n",
    "        target_cols = ['pod_cpu_percentage', 'pod_memory_percentage']\n",
    "        \n",
    "        sequences_X, sequences_y, pod_names, timestamps = [], [], [] \n",
    "                   \n",
    "        # Prepare features and targets\n",
    "        features = pod_df[feature_cols].values\n",
    "        targets = pod_df[target_cols].values\n",
    "        \n",
    "        # Create sequences\n",
    "        for i in range(len(pod_df) - sequence_length - prediction_horizon + 1):\n",
    "            X_seq = features[i:i + sequence_length]\n",
    "            y_seq = targets[i + sequence_length:i + sequence_length + prediction_horizon]\n",
    "            \n",
    "            sequences_X.append(X_seq)\n",
    "            sequences_y.append(y_seq)\n",
    "            pod_names.append(pod_name)\n",
    "            timestamps.append(pod_df.iloc[i + sequence_length]['timestamp'])\n",
    "    \n",
    "        sequences_X = np.array(sequences_X)\n",
    "        sequences_y = np.array(sequences_y)\n",
    "        \n",
    "        print(f\"Created {len(sequences_X)} sequences\")\n",
    "        print(f\"Input shape: {sequences_X.shape}\")\n",
    "        print(f\"Output shape: {sequences_y.shape}\")\n",
    "        \n",
    "        return sequences_X, sequences_y, np.array(pod_names), np.array(timestamps)\n",
    "    \n",
    "    def normalize_data(self, X_train, y_train, X_val=None, y_val=None):\n",
    "        \"\"\"Normalize features and targets\"\"\"\n",
    "        print(\"Normalizing data...\")\n",
    "        \n",
    "        # Reshape for scaling\n",
    "        X_train_reshaped = X_train.reshape(-1, X_train.shape[-1])\n",
    "        y_train_reshaped = y_train.reshape(-1, y_train.shape[-1])\n",
    "        \n",
    "        # Fit scalers on training data\n",
    "        X_train_scaled = self.scaler_features.fit_transform(X_train_reshaped)\n",
    "        y_train_scaled = self.scaler_targets.fit_transform(y_train_reshaped)\n",
    "        \n",
    "        # Reshape back\n",
    "        X_train_scaled = X_train_scaled.reshape(X_train.shape)\n",
    "        y_train_scaled = y_train_scaled.reshape(y_train.shape)\n",
    "        \n",
    "        if X_val is not None and y_val is not None:\n",
    "            X_val_reshaped = X_val.reshape(-1, X_val.shape[-1])\n",
    "            y_val_reshaped = y_val.reshape(-1, y_val.shape[-1])\n",
    "            \n",
    "            X_val_scaled = self.scaler_features.transform(X_val_reshaped)\n",
    "            y_val_scaled = self.scaler_targets.transform(y_val_reshaped)\n",
    "            \n",
    "            X_val_scaled = X_val_scaled.reshape(X_val.shape)\n",
    "            y_val_scaled = y_val_scaled.reshape(y_val.shape)\n",
    "            \n",
    "            return X_train_scaled, y_train_scaled, X_val_scaled, y_val_scaled\n",
    "        \n",
    "        return X_train_scaled, y_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T03:01:57.935300Z",
     "iopub.status.busy": "2025-06-05T03:01:57.934956Z",
     "iopub.status.idle": "2025-06-05T03:01:57.952449Z",
     "shell.execute_reply": "2025-06-05T03:01:57.951461Z",
     "shell.execute_reply.started": "2025-06-05T03:01:57.935278Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 1. DATA LOADING AND PREPROCESSING\n",
    "# =============================================================================\n",
    "\n",
    "class DataProcessor:\n",
    "    def __init__(self, csv_path):\n",
    "        self.csv_path = csv_path\n",
    "        self.scaler_features = StandardScaler()\n",
    "        self.scaler_targets = MinMaxScaler()\n",
    "        \n",
    "    def load_and_preprocess(self):\n",
    "        \"\"\"Load CSV and perform initial preprocessing\"\"\"\n",
    "        print(\"Loading data from CSV...\")\n",
    "        df = pd.read_csv(self.csv_path)\n",
    "        \n",
    "        # Convert timestamp to datetime\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'], format='%d/%m/%Y %H:%M') \n",
    "('%Y-%m-%d %H:%M')\n",
    " # pd.to_datetime(df['timestamp'])\n",
    "        \n",
    "        # Derive pod-level CPU and memory percentages\n",
    "        print(\"Deriving pod-level CPU and memory percentages...\")\n",
    "        df['pod_cpu_percentage'] = df['cpu_allocation_efficiency'] * df['node_cpu_usage']\n",
    "        df['pod_memory_percentage'] = df['memory_allocation_efficiency'] * df['node_memory_usage']\n",
    "        \n",
    "        # Clip values to reasonable ranges (0-100% for percentages)\n",
    "        df['pod_cpu_percentage'] = np.clip(df['pod_cpu_percentage'], 0, 100)\n",
    "        df['pod_memory_percentage'] = np.clip(df['pod_memory_percentage'], 0, 100)\n",
    "        \n",
    "        print(f\"Data shape: {df.shape}\")\n",
    "        print(f\"Unique pods: {df['pod_name'].nunique()}\")\n",
    "        print(f\"Date range: {df['timestamp'].min()} to {df['timestamp'].max()}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def resample_to_5min(self, df):\n",
    "        \"\"\"Resample 1-minute data to 5-minute intervals\"\"\"\n",
    "        print(\"Resampling data from 1-minute to 5-minute intervals...\")\n",
    "\n",
    "        new_df = df.copy()\n",
    "        resampled_dfs = []\n",
    "        # for pod_name in df['pod_name'].unique():\n",
    "        #     pod_df = df[df['pod_name'] == pod_name].copy()\n",
    "        #     pod_df = pod_df.set_index('timestamp')\n",
    "            \n",
    "        # Resample numeric columns\n",
    "        numeric_cols = ['cpu_allocation_efficiency', 'memory_allocation_efficiency', \n",
    "                      'disk_io', 'network_latency', 'node_temperature', \n",
    "                      'node_cpu_usage', 'node_memory_usage', 'pod_lifetime_seconds',\n",
    "                      'pod_cpu_percentage', 'pod_memory_percentage']\n",
    "        \n",
    "        resampled = new_df[numeric_cols].resample('5T').mean()\n",
    "        # resampled['pod_name'] = pod_name\n",
    "        resampled['event_type'] = pod_df['event_type'].resample('5T').first()\n",
    "        \n",
    "        # resampled_dfs.append(resampled.reset_index())\n",
    "        \n",
    "        # result = pd.concat(resampled_dfs, ignore_index=True)\n",
    "        # result = result.dropna()  # Remove any NaN values from resampling\n",
    "        \n",
    "        # print(f\"Resampled data shape: {result.shape}\")\n",
    "        # return result\n",
    "        resampled.dropna(inplace = True)\n",
    "        return resampled\n",
    "\n",
    "    def prepare_sequences(self, df, sequence_length=24, prediction_horizon=12):\n",
    "        \"\"\"\n",
    "        Prepare sequences for LSTM training\n",
    "        sequence_length: Number of past time steps to use (24 = 2 hours of 5-min data)\n",
    "        prediction_horizon: Number of future steps to predict (12 = 1 hour)\n",
    "        \"\"\"\n",
    "        print(f\"Preparing sequences (seq_len={sequence_length}, pred_horizon={prediction_horizon})...\")\n",
    "        \n",
    "        # Feature columns (excluding targets and identifiers)\n",
    "        feature_cols = ['cpu_allocation_efficiency', 'memory_allocation_efficiency', \n",
    "                       'disk_io', 'network_latency', 'node_temperature', \n",
    "                       'node_cpu_usage', 'node_memory_usage', 'pod_lifetime_seconds']\n",
    "        \n",
    "        # Target columns\n",
    "        target_cols = ['pod_cpu_percentage', 'pod_memory_percentage']\n",
    "        \n",
    "        sequences_X, sequences_y, pod_names, timestamps = [], [], [], []\n",
    "        \n",
    "        for pod_name in df['pod_name'].unique():\n",
    "            pod_df = df[df['pod_name'] == pod_name].sort_values('timestamp')\n",
    "            \n",
    "            if len(pod_df) < sequence_length + prediction_horizon:\n",
    "                continue  # Skip pods with insufficient data\n",
    "            \n",
    "            # Prepare features and targets\n",
    "            features = pod_df[feature_cols].values\n",
    "            targets = pod_df[target_cols].values\n",
    "            \n",
    "            # Create sequences\n",
    "            for i in range(len(pod_df) - sequence_length - prediction_horizon + 1):\n",
    "                X_seq = features[i:i + sequence_length]\n",
    "                y_seq = targets[i + sequence_length:i + sequence_length + prediction_horizon]\n",
    "                \n",
    "                sequences_X.append(X_seq)\n",
    "                sequences_y.append(y_seq)\n",
    "                pod_names.append(pod_name)\n",
    "                timestamps.append(pod_df.iloc[i + sequence_length]['timestamp'])\n",
    "        \n",
    "        sequences_X = np.array(sequences_X)\n",
    "        sequences_y = np.array(sequences_y)\n",
    "        \n",
    "        print(f\"Created {len(sequences_X)} sequences\")\n",
    "        print(f\"Input shape: {sequences_X.shape}\")\n",
    "        print(f\"Output shape: {sequences_y.shape}\")\n",
    "        \n",
    "        return sequences_X, sequences_y, np.array(pod_names), np.array(timestamps)\n",
    "    \n",
    "    def normalize_data(self, X_train, y_train, X_val=None, y_val=None):\n",
    "        \"\"\"Normalize features and targets\"\"\"\n",
    "        print(\"Normalizing data...\")\n",
    "        \n",
    "        # Reshape for scaling\n",
    "        X_train_reshaped = X_train.reshape(-1, X_train.shape[-1])\n",
    "        y_train_reshaped = y_train.reshape(-1, y_train.shape[-1])\n",
    "        \n",
    "        # Fit scalers on training data\n",
    "        X_train_scaled = self.scaler_features.fit_transform(X_train_reshaped)\n",
    "        y_train_scaled = self.scaler_targets.fit_transform(y_train_reshaped)\n",
    "        \n",
    "        # Reshape back\n",
    "        X_train_scaled = X_train_scaled.reshape(X_train.shape)\n",
    "        y_train_scaled = y_train_scaled.reshape(y_train.shape)\n",
    "        \n",
    "        if X_val is not None and y_val is not None:\n",
    "            X_val_reshaped = X_val.reshape(-1, X_val.shape[-1])\n",
    "            y_val_reshaped = y_val.reshape(-1, y_val.shape[-1])\n",
    "            \n",
    "            X_val_scaled = self.scaler_features.transform(X_val_reshaped)\n",
    "            y_val_scaled = self.scaler_targets.transform(y_val_reshaped)\n",
    "            \n",
    "            X_val_scaled = X_val_scaled.reshape(X_val.shape)\n",
    "            y_val_scaled = y_val_scaled.reshape(y_val.shape)\n",
    "            \n",
    "            return X_train_scaled, y_train_scaled, X_val_scaled, y_val_scaled\n",
    "        \n",
    "        return X_train_scaled, y_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T08:54:57.721840Z",
     "iopub.status.busy": "2025-06-05T08:54:57.721453Z",
     "iopub.status.idle": "2025-06-05T08:54:57.728206Z",
     "shell.execute_reply": "2025-06-05T08:54:57.726998Z",
     "shell.execute_reply.started": "2025-06-05T08:54:57.721815Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    " def load_and_preprocess(csv_path):\n",
    "    \"\"\"Load CSV and perform initial preprocessing\"\"\"\n",
    "    print(\"Loading data from CSV...\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Convert timestamp to datetime\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], format='%d/%m/%Y %H:%M')\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T08:55:55.502074Z",
     "iopub.status.busy": "2025-06-05T08:55:55.501017Z",
     "iopub.status.idle": "2025-06-05T08:55:55.575929Z",
     "shell.execute_reply": "2025-06-05T08:55:55.574752Z",
     "shell.execute_reply.started": "2025-06-05T08:55:55.502042Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>pod_name</th>\n",
       "      <th>namespace</th>\n",
       "      <th>cpu_allocation_efficiency</th>\n",
       "      <th>memory_allocation_efficiency</th>\n",
       "      <th>disk_io</th>\n",
       "      <th>network_latency</th>\n",
       "      <th>node_temperature</th>\n",
       "      <th>node_cpu_usage</th>\n",
       "      <th>node_memory_usage</th>\n",
       "      <th>event_type</th>\n",
       "      <th>event_message</th>\n",
       "      <th>scaling_event</th>\n",
       "      <th>pod_lifetime_seconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01/01/2023 00:00</td>\n",
       "      <td>pod_0</td>\n",
       "      <td>dev</td>\n",
       "      <td>0.038162</td>\n",
       "      <td>0.949259</td>\n",
       "      <td>9.993579</td>\n",
       "      <td>13.722542</td>\n",
       "      <td>77.619073</td>\n",
       "      <td>93.177619</td>\n",
       "      <td>37.900532</td>\n",
       "      <td>Warning</td>\n",
       "      <td>Killed</td>\n",
       "      <td>False</td>\n",
       "      <td>119648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01/01/2023 00:00</td>\n",
       "      <td>pod_1</td>\n",
       "      <td>default</td>\n",
       "      <td>0.500763</td>\n",
       "      <td>0.048543</td>\n",
       "      <td>935.792442</td>\n",
       "      <td>55.493953</td>\n",
       "      <td>84.182245</td>\n",
       "      <td>61.442289</td>\n",
       "      <td>5.208161</td>\n",
       "      <td>Error</td>\n",
       "      <td>Failed</td>\n",
       "      <td>True</td>\n",
       "      <td>144516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01/01/2023 00:00</td>\n",
       "      <td>pod_2</td>\n",
       "      <td>kube-system</td>\n",
       "      <td>0.746726</td>\n",
       "      <td>0.447345</td>\n",
       "      <td>328.352359</td>\n",
       "      <td>173.910016</td>\n",
       "      <td>21.295244</td>\n",
       "      <td>55.819311</td>\n",
       "      <td>18.335802</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Completed</td>\n",
       "      <td>True</td>\n",
       "      <td>68857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01/01/2023 00:00</td>\n",
       "      <td>pod_3</td>\n",
       "      <td>default</td>\n",
       "      <td>0.526692</td>\n",
       "      <td>0.870251</td>\n",
       "      <td>778.297708</td>\n",
       "      <td>67.395729</td>\n",
       "      <td>85.028829</td>\n",
       "      <td>78.968463</td>\n",
       "      <td>94.619689</td>\n",
       "      <td>Warning</td>\n",
       "      <td>OOMKilled</td>\n",
       "      <td>True</td>\n",
       "      <td>72080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01/01/2023 00:00</td>\n",
       "      <td>pod_4</td>\n",
       "      <td>prod</td>\n",
       "      <td>0.425342</td>\n",
       "      <td>0.885459</td>\n",
       "      <td>711.181295</td>\n",
       "      <td>91.724730</td>\n",
       "      <td>29.157695</td>\n",
       "      <td>52.718141</td>\n",
       "      <td>70.770594</td>\n",
       "      <td>Error</td>\n",
       "      <td>Killed</td>\n",
       "      <td>False</td>\n",
       "      <td>123016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          timestamp pod_name    namespace  cpu_allocation_efficiency  \\\n",
       "0  01/01/2023 00:00    pod_0          dev                   0.038162   \n",
       "1  01/01/2023 00:00    pod_1      default                   0.500763   \n",
       "2  01/01/2023 00:00    pod_2  kube-system                   0.746726   \n",
       "3  01/01/2023 00:00    pod_3      default                   0.526692   \n",
       "4  01/01/2023 00:00    pod_4         prod                   0.425342   \n",
       "\n",
       "   memory_allocation_efficiency     disk_io  network_latency  \\\n",
       "0                      0.949259    9.993579        13.722542   \n",
       "1                      0.048543  935.792442        55.493953   \n",
       "2                      0.447345  328.352359       173.910016   \n",
       "3                      0.870251  778.297708        67.395729   \n",
       "4                      0.885459  711.181295        91.724730   \n",
       "\n",
       "   node_temperature  node_cpu_usage  node_memory_usage event_type  \\\n",
       "0         77.619073       93.177619          37.900532    Warning   \n",
       "1         84.182245       61.442289           5.208161      Error   \n",
       "2         21.295244       55.819311          18.335802     Normal   \n",
       "3         85.028829       78.968463          94.619689    Warning   \n",
       "4         29.157695       52.718141          70.770594      Error   \n",
       "\n",
       "  event_message  scaling_event  pod_lifetime_seconds  \n",
       "0        Killed          False                119648  \n",
       "1        Failed           True                144516  \n",
       "2     Completed           True                 68857  \n",
       "3     OOMKilled           True                 72080  \n",
       "4        Killed          False                123016  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_path = \"/data/kaggle/input/kubernetes_performance_metrics_dataset.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T08:59:17.466680Z",
     "iopub.status.busy": "2025-06-05T08:59:17.466289Z",
     "iopub.status.idle": "2025-06-05T08:59:17.476680Z",
     "shell.execute_reply": "2025-06-05T08:59:17.475610Z",
     "shell.execute_reply.started": "2025-06-05T08:59:17.466655Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df['timestamp'] = pd.to_datetime(df['timestamp'], format='%d/%m/%Y %H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T08:59:44.994009Z",
     "iopub.status.busy": "2025-06-05T08:59:44.993594Z",
     "iopub.status.idle": "2025-06-05T08:59:45.015403Z",
     "shell.execute_reply": "2025-06-05T08:59:45.014112Z",
     "shell.execute_reply.started": "2025-06-05T08:59:44.993979Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>pod_name</th>\n",
       "      <th>namespace</th>\n",
       "      <th>cpu_allocation_efficiency</th>\n",
       "      <th>memory_allocation_efficiency</th>\n",
       "      <th>disk_io</th>\n",
       "      <th>network_latency</th>\n",
       "      <th>node_temperature</th>\n",
       "      <th>node_cpu_usage</th>\n",
       "      <th>node_memory_usage</th>\n",
       "      <th>event_type</th>\n",
       "      <th>event_message</th>\n",
       "      <th>scaling_event</th>\n",
       "      <th>pod_lifetime_seconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14995</th>\n",
       "      <td>2023-01-01 04:09:00</td>\n",
       "      <td>pod_14995</td>\n",
       "      <td>kube-system</td>\n",
       "      <td>0.020767</td>\n",
       "      <td>0.697208</td>\n",
       "      <td>379.511285</td>\n",
       "      <td>147.031290</td>\n",
       "      <td>66.820729</td>\n",
       "      <td>23.681710</td>\n",
       "      <td>65.269283</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Started</td>\n",
       "      <td>True</td>\n",
       "      <td>111871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14996</th>\n",
       "      <td>2023-01-01 04:09:00</td>\n",
       "      <td>pod_14996</td>\n",
       "      <td>default</td>\n",
       "      <td>0.026490</td>\n",
       "      <td>0.973705</td>\n",
       "      <td>93.014634</td>\n",
       "      <td>80.106794</td>\n",
       "      <td>21.358463</td>\n",
       "      <td>22.612871</td>\n",
       "      <td>48.674617</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Started</td>\n",
       "      <td>False</td>\n",
       "      <td>64848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14997</th>\n",
       "      <td>2023-01-01 04:09:00</td>\n",
       "      <td>pod_14997</td>\n",
       "      <td>dev</td>\n",
       "      <td>0.321295</td>\n",
       "      <td>0.073787</td>\n",
       "      <td>112.686558</td>\n",
       "      <td>83.621580</td>\n",
       "      <td>70.648406</td>\n",
       "      <td>74.516728</td>\n",
       "      <td>76.802551</td>\n",
       "      <td>Error</td>\n",
       "      <td>Failed</td>\n",
       "      <td>True</td>\n",
       "      <td>126843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14998</th>\n",
       "      <td>2023-01-01 04:09:00</td>\n",
       "      <td>pod_14998</td>\n",
       "      <td>kube-system</td>\n",
       "      <td>0.087156</td>\n",
       "      <td>0.322506</td>\n",
       "      <td>804.890194</td>\n",
       "      <td>158.398994</td>\n",
       "      <td>41.005118</td>\n",
       "      <td>58.788146</td>\n",
       "      <td>53.529527</td>\n",
       "      <td>Warning</td>\n",
       "      <td>Started</td>\n",
       "      <td>False</td>\n",
       "      <td>137157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14999</th>\n",
       "      <td>2023-01-01 04:09:00</td>\n",
       "      <td>pod_14999</td>\n",
       "      <td>default</td>\n",
       "      <td>0.094542</td>\n",
       "      <td>0.052845</td>\n",
       "      <td>624.286513</td>\n",
       "      <td>74.228015</td>\n",
       "      <td>21.776262</td>\n",
       "      <td>14.834679</td>\n",
       "      <td>32.077468</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Killed</td>\n",
       "      <td>True</td>\n",
       "      <td>112793</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                timestamp   pod_name    namespace  cpu_allocation_efficiency  \\\n",
       "14995 2023-01-01 04:09:00  pod_14995  kube-system                   0.020767   \n",
       "14996 2023-01-01 04:09:00  pod_14996      default                   0.026490   \n",
       "14997 2023-01-01 04:09:00  pod_14997          dev                   0.321295   \n",
       "14998 2023-01-01 04:09:00  pod_14998  kube-system                   0.087156   \n",
       "14999 2023-01-01 04:09:00  pod_14999      default                   0.094542   \n",
       "\n",
       "       memory_allocation_efficiency     disk_io  network_latency  \\\n",
       "14995                      0.697208  379.511285       147.031290   \n",
       "14996                      0.973705   93.014634        80.106794   \n",
       "14997                      0.073787  112.686558        83.621580   \n",
       "14998                      0.322506  804.890194       158.398994   \n",
       "14999                      0.052845  624.286513        74.228015   \n",
       "\n",
       "       node_temperature  node_cpu_usage  node_memory_usage event_type  \\\n",
       "14995         66.820729       23.681710          65.269283     Normal   \n",
       "14996         21.358463       22.612871          48.674617     Normal   \n",
       "14997         70.648406       74.516728          76.802551      Error   \n",
       "14998         41.005118       58.788146          53.529527    Warning   \n",
       "14999         21.776262       14.834679          32.077468     Normal   \n",
       "\n",
       "      event_message  scaling_event  pod_lifetime_seconds  \n",
       "14995       Started           True                111871  \n",
       "14996       Started          False                 64848  \n",
       "14997        Failed           True                126843  \n",
       "14998       Started          False                137157  \n",
       "14999        Killed           True                112793  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T09:00:32.621811Z",
     "iopub.status.busy": "2025-06-05T09:00:32.621408Z",
     "iopub.status.idle": "2025-06-05T09:00:32.631761Z",
     "shell.execute_reply": "2025-06-05T09:00:32.630759Z",
     "shell.execute_reply.started": "2025-06-05T09:00:32.621786Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"pod_name\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T03:01:58.221786Z",
     "iopub.status.busy": "2025-06-05T03:01:58.220962Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from CSV...\n",
      "Deriving pod-level CPU and memory percentages...\n",
      "Data shape: (15000, 16)\n",
      "Unique pods: 15000\n",
      "Date range: 2023-01-01 00:00:00 to 2023-01-01 04:09:00\n",
      "Resampling data from 1-minute to 5-minute intervals...\n",
      "Resampled data shape: (15000, 13)\n"
     ]
    }
   ],
   "source": [
    "csv_path = \"/kaggle/input/kubernetes-resource-and-performancemetricsallocation/kubernetes_performance_metrics_dataset.csv\"\n",
    "processor = DataProcessor(csv_path)\n",
    "\n",
    "# Load and preprocess data\n",
    "df = processor.load_and_preprocess()\n",
    "\n",
    "# Resample to 5-minute intervals\n",
    "df_resampled = processor.resample_to_5min(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_resampled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_resa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Prepare sequences\n",
    "X, y, pod_names, timestamps = processor.prepare_sequences(df_resampled)\n",
    "\n",
    "# Train/validation split (80/20)\n",
    "split_idx = int(0.8 * len(X))\n",
    "\n",
    "X_train, X_val = X[:split_idx], X[split_idx:]\n",
    "y_train, y_val = y[:split_idx], y[split_idx:]\n",
    "\n",
    "# Normalize data\n",
    "X_train_scaled, y_train_scaled, X_val_scaled, y_val_scaled = processor.normalize_data(\n",
    "    X_train, y_train, X_val, y_val\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 2. LSTM MODEL ARCHITECTURE\n",
    "# =============================================================================\n",
    "\n",
    "class MultiOutputLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, prediction_horizon, dropout=0.2):\n",
    "        super(MultiOutputLSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.prediction_horizon = prediction_horizon\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Output layer that produces all future predictions at once\n",
    "        self.fc = nn.Linear(hidden_size, output_size * prediction_horizon)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                torch.nn.init.xavier_uniform_(param.data)\n",
    "            elif 'weight_hh' in name:\n",
    "                torch.nn.init.orthogonal_(param.data)\n",
    "            elif 'bias' in name:\n",
    "                param.data.fill_(0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Initialize hidden state\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "        \n",
    "        # LSTM forward pass\n",
    "        lstm_out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Use the last output\n",
    "        last_output = lstm_out[:, -1, :]\n",
    "        last_output = self.dropout(last_output)\n",
    "        \n",
    "        # Generate predictions\n",
    "        output = self.fc(last_output)\n",
    "        \n",
    "        # Reshape to (batch_size, prediction_horizon, output_size)\n",
    "        output = output.view(batch_size, self.prediction_horizon, self.output_size)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# =============================================================================\n",
    "# 3. TRAINING PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "class LSTMTrainer:\n",
    "    def __init__(self, model, device='cpu'):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.training_history = {'train_loss': [], 'val_loss': []}\n",
    "    \n",
    "    def train_model(self, X_train, y_train, X_val, y_val, epochs=100, batch_size=32, lr=0.001):\n",
    "        \"\"\"Train the LSTM model\"\"\"\n",
    "        print(f\"Training model on {self.device}...\")\n",
    "        \n",
    "        # Convert to tensors\n",
    "        X_train = torch.FloatTensor(X_train).to(self.device)\n",
    "        y_train = torch.FloatTensor(y_train).to(self.device)\n",
    "        X_val = torch.FloatTensor(X_val).to(self.device)\n",
    "        y_val = torch.FloatTensor(y_val).to(self.device)\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        # Optimizer and loss function\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.5)\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        patience = 15\n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Training\n",
    "            self.model.train()\n",
    "            train_loss = 0.0\n",
    "            \n",
    "            for batch_X, batch_y in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                predictions = self.model(batch_X)\n",
    "                loss = criterion(predictions, batch_y)\n",
    "                loss.backward()\n",
    "                \n",
    "                # Gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                \n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "            \n",
    "            # Validation\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_predictions = self.model(X_val)\n",
    "                val_loss = criterion(val_predictions, y_val).item()\n",
    "            \n",
    "            # Learning rate scheduling\n",
    "            scheduler.step(val_loss)\n",
    "            \n",
    "            # Track history\n",
    "            avg_train_loss = train_loss / len(train_loader)\n",
    "            self.training_history['train_loss'].append(avg_train_loss)\n",
    "            self.training_history['val_loss'].append(val_loss)\n",
    "            \n",
    "            # Early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                # Save best model\n",
    "                torch.save(self.model.state_dict(), 'best_model.pth')\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                    break\n",
    "            \n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {avg_train_loss:.6f}, Val Loss: {val_loss:.6f}')\n",
    "        \n",
    "        # Load best model\n",
    "        self.model.load_state_dict(torch.load('best_model.pth'))\n",
    "        print(\"Training completed!\")\n",
    "        \n",
    "        return self.training_history\n",
    "    \n",
    "    def plot_training_history(self):\n",
    "        \"\"\"Plot training and validation loss\"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(self.training_history['train_loss'], label='Training Loss')\n",
    "        plt.plot(self.training_history['val_loss'], label='Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training History')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 4. DISASTER PREDICTION AND ALERTING SYSTEM\n",
    "# =============================================================================\n",
    "\n",
    "class DisasterPredictor:\n",
    "    def __init__(self, model, scaler_features, scaler_targets, threshold=0.7, device='cpu'):\n",
    "        self.model = model.to(device)\n",
    "        self.scaler_features = scaler_features\n",
    "        self.scaler_targets = scaler_targets\n",
    "        self.threshold = threshold\n",
    "        self.device = device\n",
    "        self.model.eval()\n",
    "    \n",
    "    def predict_single_pod(self, pod_data):\n",
    "        \"\"\"\n",
    "        Predict future CPU/Memory usage for a single pod\n",
    "        pod_data: numpy array of shape (sequence_length, n_features)\n",
    "        \"\"\"\n",
    "        # Normalize input\n",
    "        pod_data_normalized = self.scaler_features.transform(pod_data)\n",
    "        \n",
    "        # Convert to tensor\n",
    "        X = torch.FloatTensor(pod_data_normalized).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            predictions = self.model(X)\n",
    "            predictions = predictions.cpu().numpy().squeeze()\n",
    "        \n",
    "        # Denormalize predictions\n",
    "        predictions_reshaped = predictions.reshape(-1, predictions.shape[-1])\n",
    "        predictions_denormalized = self.scaler_targets.inverse_transform(predictions_reshaped)\n",
    "        predictions_final = predictions_denormalized.reshape(predictions.shape)\n",
    "        \n",
    "        return predictions_final\n",
    "    \n",
    "    def check_disaster_conditions(self, predictions, pod_name, current_time):\n",
    "        \"\"\"\n",
    "        Check if predictions exceed disaster threshold\n",
    "        Returns alert information if disaster conditions are met\n",
    "        \"\"\"\n",
    "        cpu_predictions = predictions[:, 0]  # CPU percentage predictions\n",
    "        memory_predictions = predictions[:, 1]  # Memory percentage predictions\n",
    "        \n",
    "        # Convert to percentage (0-1 to 0-100 if needed)\n",
    "        if cpu_predictions.max() <= 1.0:\n",
    "            cpu_predictions *= 100\n",
    "            memory_predictions *= 100\n",
    "        \n",
    "        alerts = []\n",
    "        \n",
    "        # Check CPU threshold\n",
    "        cpu_breach_indices = np.where(cpu_predictions > self.threshold * 100)[0]\n",
    "        if len(cpu_breach_indices) > 0:\n",
    "            first_breach = cpu_breach_indices[0]\n",
    "            time_to_breach = first_breach * 5  # 5 minutes per prediction step\n",
    "            \n",
    "            alerts.append({\n",
    "                'pod_name': pod_name,\n",
    "                'metric': 'CPU',\n",
    "                'predicted_value': cpu_predictions[first_breach],\n",
    "                'threshold': self.threshold * 100,\n",
    "                'time_until_breach_minutes': time_to_breach,\n",
    "                'breach_time': current_time + timedelta(minutes=time_to_breach),\n",
    "                'severity': 'HIGH' if cpu_predictions[first_breach] > 90 else 'MEDIUM'\n",
    "            })\n",
    "        \n",
    "        # Check Memory threshold\n",
    "        memory_breach_indices = np.where(memory_predictions > self.threshold * 100)[0]\n",
    "        if len(memory_breach_indices) > 0:\n",
    "            first_breach = memory_breach_indices[0]\n",
    "            time_to_breach = first_breach * 5\n",
    "            \n",
    "            alerts.append({\n",
    "                'pod_name': pod_name,\n",
    "                'metric': 'Memory',\n",
    "                'predicted_value': memory_predictions[first_breach],\n",
    "                'threshold': self.threshold * 100,\n",
    "                'time_until_breach_minutes': time_to_breach,\n",
    "                'breach_time': current_time + timedelta(minutes=time_to_breach),\n",
    "                'severity': 'HIGH' if memory_predictions[first_breach] > 90 else 'MEDIUM'\n",
    "            })\n",
    "        \n",
    "        return alerts\n",
    "    \n",
    "    def send_slack_alert(self, alerts, webhook_url):\n",
    "        \"\"\"Send disaster alert to Slack\"\"\"\n",
    "        if not alerts:\n",
    "            return\n",
    "        \n",
    "        for alert in alerts:\n",
    "            color = '#ff0000' if alert['severity'] == 'HIGH' else '#ff9900'\n",
    "            \n",
    "            message = {\n",
    "                \"attachments\": [\n",
    "                    {\n",
    "                        \"color\": color,\n",
    "                        \"title\": f\"🚨 K8s Disaster Alert - {alert['severity']} Priority\",\n",
    "                        \"fields\": [\n",
    "                            {\n",
    "                                \"title\": \"Pod Name\",\n",
    "                                \"value\": alert['pod_name'],\n",
    "                                \"short\": True\n",
    "                            },\n",
    "                            {\n",
    "                                \"title\": \"Metric\",\n",
    "                                \"value\": alert['metric'],\n",
    "                                \"short\": True\n",
    "                            },\n",
    "                            {\n",
    "                                \"title\": \"Predicted Value\",\n",
    "                                \"value\": f\"{alert['predicted_value']:.1f}%\",\n",
    "                                \"short\": True\n",
    "                            },\n",
    "                            {\n",
    "                                \"title\": \"Threshold\",\n",
    "                                \"value\": f\"{alert['threshold']:.0f}%\",\n",
    "                                \"short\": True\n",
    "                            },\n",
    "                            {\n",
    "                                \"title\": \"Time Until Breach\",\n",
    "                                \"value\": f\"{alert['time_until_breach_minutes']} minutes\",\n",
    "                                \"short\": True\n",
    "                            },\n",
    "                            {\n",
    "                                \"title\": \"Expected Breach Time\",\n",
    "                                \"value\": alert['breach_time'].strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                                \"short\": True\n",
    "                            }\n",
    "                        ],\n",
    "                        \"footer\": \"K8s LSTM Disaster Recovery System\",\n",
    "                        \"ts\": int(datetime.now().timestamp())\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                response = requests.post(webhook_url, json=message)\n",
    "                response.raise_for_status()\n",
    "                print(f\"Alert sent for {alert['pod_name']} - {alert['metric']}\")\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Failed to send Slack alert: {e}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 5. MAIN EXECUTION PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "def main_pipeline(csv_path, slack_webhook_url=None):\n",
    "    \"\"\"Main execution pipeline for disaster recovery system\"\"\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"KUBERNETES LSTM DISASTER RECOVERY SYSTEM\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. Data Processing\n",
    "    processor = DataProcessor(csv_path)\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    df = processor.load_and_preprocess()\n",
    "    \n",
    "    # Resample to 5-minute intervals\n",
    "    df_resampled = processor.resample_to_5min(df)\n",
    "    \n",
    "    # Prepare sequences\n",
    "    X, y, pod_names, timestamps = processor.prepare_sequences(df_resampled)\n",
    "    \n",
    "    # Train/validation split (80/20)\n",
    "    split_idx = int(0.8 * len(X))\n",
    "    \n",
    "    X_train, X_val = X[:split_idx], X[split_idx:]\n",
    "    y_train, y_val = y[:split_idx], y[split_idx:]\n",
    "    \n",
    "    # Normalize data\n",
    "    X_train_scaled, y_train_scaled, X_val_scaled, y_val_scaled = processor.normalize_data(\n",
    "        X_train, y_train, X_val, y_val\n",
    "    )\n",
    "    \n",
    "    print(f\"Training samples: {len(X_train_scaled)}\")\n",
    "    print(f\"Validation samples: {len(X_val_scaled)}\")\n",
    "    \n",
    "    # 2. Model Training\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Model parameters\n",
    "    input_size = X_train_scaled.shape[2]  # Number of features\n",
    "    hidden_size = 64\n",
    "    num_layers = 2\n",
    "    output_size = 2  # CPU and Memory\n",
    "    prediction_horizon = 12  # 1 hour of 5-minute predictions\n",
    "    \n",
    "    # Initialize model\n",
    "    model = MultiOutputLSTM(\n",
    "        input_size=input_size,\n",
    "        hidden_size=hidden_size,\n",
    "        num_layers=num_layers,\n",
    "        output_size=output_size,\n",
    "        prediction_horizon=prediction_horizon,\n",
    "        dropout=0.3\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    trainer = LSTMTrainer(model, device)\n",
    "    history = trainer.train_model(\n",
    "        X_train_scaled, y_train_scaled, \n",
    "        X_val_scaled, y_val_scaled,\n",
    "        epochs=100, batch_size=32, lr=0.001\n",
    "    )\n",
    "    \n",
    "    # Plot training history\n",
    "    trainer.plot_training_history()\n",
    "    \n",
    "    # 3. Model Evaluation\n",
    "    print(\"\\n\" + \"=\" * 40)\n",
    "    print(\"MODEL EVALUATION\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Make predictions on validation set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_val_tensor = torch.FloatTensor(X_val_scaled).to(device)\n",
    "        val_predictions = model(X_val_tensor).cpu().numpy()\n",
    "    \n",
    "    # Denormalize predictions and targets\n",
    "    val_pred_reshaped = val_predictions.reshape(-1, val_predictions.shape[-1])\n",
    "    val_true_reshaped = y_val_scaled.reshape(-1, y_val_scaled.shape[-1])\n",
    "    \n",
    "    val_pred_denorm = processor.scaler_targets.inverse_transform(val_pred_reshaped)\n",
    "    val_true_denorm = processor.scaler_targets.inverse_transform(val_true_reshaped)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse_cpu = mean_squared_error(val_true_denorm[:, 0], val_pred_denorm[:, 0])\n",
    "    mae_cpu = mean_absolute_error(val_true_denorm[:, 0], val_pred_denorm[:, 0])\n",
    "    \n",
    "    mse_memory = mean_squared_error(val_true_denorm[:, 1], val_pred_denorm[:, 1])\n",
    "    mae_memory = mean_absolute_error(val_true_denorm[:, 1], val_pred_denorm[:, 1])\n",
    "    \n",
    "    print(f\"CPU Prediction - MSE: {mse_cpu:.4f}, MAE: {mae_cpu:.4f}\")\n",
    "    print(f\"Memory Prediction - MSE: {mse_memory:.4f}, MAE: {mae_memory:.4f}\")\n",
    "    \n",
    "    # 4. Disaster Prediction System\n",
    "    print(\"\\n\" + \"=\" * 40)\n",
    "    print(\"DISASTER PREDICTION SYSTEM\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    disaster_predictor = DisasterPredictor(\n",
    "        model=model,\n",
    "        scaler_features=processor.scaler_features,\n",
    "        scaler_targets=processor.scaler_targets,\n",
    "        threshold=0.7,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Test disaster prediction on a sample\n",
    "    if len(X_val) > 0:\n",
    "        sample_idx = 0\n",
    "        sample_pod = pod_names[split_idx + sample_idx]\n",
    "        sample_data = X_val[sample_idx]\n",
    "        \n",
    "        print(f\"Testing disaster prediction for pod: {sample_pod}\")\n",
    "        \n",
    "        # Make prediction\n",
    "        predictions = disaster_predictor.predict_single_pod(sample_data)\n",
    "        \n",
    "        # Check for disaster conditions\n",
    "        current_time = datetime.now()\n",
    "        alerts = disaster_predictor.check_disaster_conditions(\n",
    "            predictions, sample_pod, current_time\n",
    "        )\n",
    "        \n",
    "        if alerts:\n",
    "            print(f\"⚠️  DISASTER CONDITIONS DETECTED for {sample_pod}!\")\n",
    "            for alert in alerts:\n",
    "                print(f\"  - {alert['metric']}: {alert['predicted_value']:.1f}% \"\n",
    "                      f\"(threshold: {alert['threshold']:.0f}%) \"\n",
    "                      f\"in {alert['time_until_breach_minutes']} minutes\")\n",
    "            \n",
    "            # Send Slack alert if webhook provided\n",
    "            if slack_webhook_url:\n",
    "                disaster_predictor.send_slack_alert(alerts, slack_webhook_url)\n",
    "        else:\n",
    "            print(f\"✅ No disaster conditions detected for {sample_pod}\")\n",
    "    \n",
    "    # 5. Save Model and Components\n",
    "    print(\"\\n\" + \"=\" * 40)\n",
    "    print(\"SAVING MODEL COMPONENTS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), 'k8s_lstm_model.pth')\n",
    "    \n",
    "    # Save scalers\n",
    "    with open('feature_scaler.pkl', 'wb') as f:\n",
    "        pickle.dump(processor.scaler_features, f)\n",
    "    \n",
    "    with open('target_scaler.pkl', 'wb') as f:\n",
    "        pickle.dump(processor.scaler_targets, f)\n",
    "    \n",
    "    # Save model configuration\n",
    "    model_config = {\n",
    "        'input_size': input_size,\n",
    "        'hidden_size': hidden_size,\n",
    "        'num_layers': num_layers,\n",
    "        'output_size': output_size,\n",
    "        'prediction_horizon': prediction_horizon,\n",
    "        'threshold': 0.7\n",
    "    }\n",
    "    \n",
    "    with open('model_config.json', 'w') as f:\n",
    "        json.dump(model_config, f)\n",
    "    \n",
    "    print(\"Model and components saved successfully!\")\n",
    "    \n",
    "    return model, processor, disaster_predictor\n",
    "\n",
    "# =============================================================================\n",
    "# 6. DEPLOYMENT UTILITIES\n",
    "# =============================================================================\n",
    "\n",
    "def load_model_for_deployment(model_path='k8s_lstm_model.pth', \n",
    "                             config_path='model_config.json',\n",
    "                             feature_scaler_path='feature_scaler.pkl',\n",
    "                             target_scaler_path='target_scaler.pkl'):\n",
    "    \"\"\"Load trained model and components for deployment\"\"\"\n",
    "    \n",
    "    # Load configuration\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    # Load scalers\n",
    "    with open(feature_scaler_path, 'rb') as f:\n",
    "        feature_scaler = pickle.load(f)\n",
    "    \n",
    "    with open(target_scaler_path, 'rb') as f:\n",
    "        target_scaler = pickle.load(f)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = MultiOutputLSTM(\n",
    "        input_size=config['input_size'],\n",
    "        hidden_size=config['hidden_size'],\n",
    "        num_layers=config['num_layers'],\n",
    "        output_size=config['output_size'],\n",
    "        prediction_horizon=config['prediction_horizon']\n",
    "    )\n",
    "    \n",
    "    # Load trained weights\n",
    "    model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize disaster predictor\n",
    "    disaster_predictor = DisasterPredictor(\n",
    "        model=model,\n",
    "        scaler_features=feature_scaler,\n",
    "        scaler_targets=target_scaler,\n",
    "        threshold=config['threshold']\n",
    "    )\n",
    "    \n",
    "    return disaster_predictor\n",
    "\n",
    "def process_new_data_batch(disaster_predictor, csv_path, slack_webhook_url=None):\n",
    "    \"\"\"Process new CSV data for real-time monitoring\"\"\"\n",
    "    \n",
    "    print(f\"Processing new data batch: {csv_path}\")\n",
    "    \n",
    "    # Load new data\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    \n",
    "    # Derive pod percentages\n",
    "    df['pod_cpu_percentage'] = df['cpu_allocation_efficiency'] * df['node_cpu_usage']\n",
    "    df['pod_memory_percentage'] = df['memory_allocation_efficiency'] * df['node_memory_usage']\n",
    "    \n",
    "    # Feature columns\n",
    "    feature_cols = ['cpu_allocation_efficiency', 'memory_allocation_efficiency', \n",
    "                   'disk_io', 'network_latency', 'node_temperature', \n",
    "                   'node_cpu_usage', 'node_memory_usage', 'pod_lifesycle_seconds']\n",
    "    \n",
    "    all_alerts = []\n",
    "    \n",
    "    # Process each pod\n",
    "    for pod_name in df['pod_name'].unique():\n",
    "        pod_df = df[df['pod_name'] == pod_name].sort_values('timestamp')\n",
    "        \n",
    "        if len(pod_df) < 24:  # Need at least 24 time steps (2 hours)\n",
    "            continue\n",
    "        \n",
    "        # Get latest 24 data points\n",
    "        latest_data = pod_df[feature_cols].tail(24).values\n",
    "        \n",
    "        # Make prediction\n",
    "        try:\n",
    "            predictions = disaster_predictor.predict_single_pod(latest_data)\n",
    "            \n",
    "            # Check for alerts\n",
    "            current_time = datetime.now()\n",
    "            alerts = disaster_predictor.check_disaster_conditions(\n",
    "                predictions, pod_name, current_time\n",
    "            )\n",
    "            \n",
    "            all_alerts.extend(alerts)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing pod {pod_name}: {e}\")\n",
    "    \n",
    "    # Send alerts\n",
    "    if all_alerts and slack_webhook_url:\n",
    "        disaster_predictor.send_slack_alert(all_alerts, slack_webhook_url)\n",
    "        print(f\"Sent {len(all_alerts)} alerts to Slack\")\n",
    "    \n",
    "    return all_alerts\n",
    "\n",
    "# =============================================================================\n",
    "# 7. EXAMPLE USAGE AND TESTING\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 4. DISASTER PREDICTION AND ALERTING SYSTEM\n",
    "# =============================================================================\n",
    "\n",
    "class DisasterPredictor:\n",
    "    def __init__(self, model, scaler_features, scaler_targets, threshold=0.7, device='cpu'):\n",
    "        self.model = model.to(device)\n",
    "        self.scaler_features = scaler_features\n",
    "        self.scaler_targets = scaler_targets\n",
    "        self.threshold = threshold\n",
    "        self.device = device\n",
    "        self.model.eval()\n",
    "    \n",
    "    def predict_cluster_usage(self, cluster_data):\n",
    "        \"\"\"\n",
    "        Predict future CPU/Memory usage for the cluster\n",
    "        cluster_data: numpy array of shape (sequence_length, n_features)\n",
    "        \"\"\"\n",
    "        # Normalize input\n",
    "        cluster_data_normalized = self.scaler_features.transform(cluster_data)\n",
    "        \n",
    "        # Convert to tensor\n",
    "        X = torch.FloatTensor(cluster_data_normalized).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            predictions = self.model(X)\n",
    "            predictions = predictions.cpu().numpy().squeeze()\n",
    "        \n",
    "        # Denormalize predictions\n",
    "        predictions_reshaped = predictions.reshape(-1, predictions.shape[-1])\n",
    "        predictions_denormalized = self.scaler_targets.inverse_transform(predictions_reshaped)\n",
    "        predictions_final = predictions_denormalized.reshape(predictions.shape)\n",
    "        \n",
    "        return predictions_final\n",
    "    \n",
    "    def check_disaster_conditions(self, predictions, current_time):\n",
    "        \"\"\"\n",
    "        Check if predictions exceed disaster threshold for cluster\n",
    "        Returns alert information if disaster conditions are met\n",
    "        \"\"\"\n",
    "        cpu_predictions = predictions[:, 0]  # CPU percentage predictions\n",
    "        memory_predictions = predictions[:, 1]  # Memory percentage predictions\n",
    "        \n",
    "        # Convert to percentage (0-1 to 0-100 if needed)\n",
    "        if cpu_predictions.max() <= 1.0:\n",
    "            cpu_predictions *= 100\n",
    "            memory_predictions *= 100\n",
    "        \n",
    "        alerts = []\n",
    "        \n",
    "        # Check CPU threshold\n",
    "        cpu_breach_indices = np.where(cpu_predictions > self.threshold * 100)[0]\n",
    "        if len(cpu_breach_indices) > 0:\n",
    "            first_breach = cpu_breach_indices[0]\n",
    "            time_to_breach = first_breach * 5  # 5 minutes per prediction step\n",
    "            max_cpu = cpu_predictions[cpu_breach_indices].max()\n",
    "            \n",
    "            alerts.append({\n",
    "                'metric': 'Cluster CPU',\n",
    "                'predicted_value': cpu_predictions[first_breach],\n",
    "                'max_predicted_value': max_cpu,\n",
    "                'threshold': self.threshold * 100,\n",
    "                'time_until_breach_minutes': time_to_breach,\n",
    "                'breach_time': current_time + timedelta(minutes=time_to_breach),\n",
    "                'severity': 'CRITICAL' if max_cpu > 90 else 'HIGH' if max_cpu > 80 else 'MEDIUM',\n",
    "                'affected_timespan_minutes': len(cpu_breach_indices) * 5\n",
    "            })\n",
    "        \n",
    "        # Check Memory threshold\n",
    "        memory_breach_indices = np.where(memory_predictions > self.threshold * 100)[0]\n",
    "        if len(memory_breach_indices) > 0:\n",
    "            first_breach = memory_breach_indices[0]\n",
    "            time_to_breach = first_breach * 5\n",
    "            max_memory = memory_predictions[memory_breach_indices].max()\n",
    "            \n",
    "            alerts.append({\n",
    "                'metric': 'Cluster Memory',\n",
    "                'predicted_value': memory_predictions[first_breach],\n",
    "                'max_predicted_value': max_memory,\n",
    "                'threshold': self.threshold * 100,\n",
    "                'time_until_breach_minutes': time_to_breach,\n",
    "                'breach_time': current_time + timedelta(minutes=time_to_breach),\n",
    "                'severity': 'CRITICAL' if max_memory > 95 else 'HIGH' if max_memory > 85 else 'MEDIUM',\n",
    "                'affected_timespan_minutes': len(memory_breach_indices) * 5\n",
    "            })\n",
    "        \n",
    "        return alerts\n",
    "    \n",
    "    def send_slack_alert(self, alerts, webhook_url):\n",
    "        \"\"\"Send disaster alert to Slack\"\"\"\n",
    "        if not alerts:\n",
    "            return\n",
    "        \n",
    "        for alert in alerts:\n",
    "            # Color coding based on severity\n",
    "            color_map = {'CRITICAL': '#ff0000', 'HIGH': '#ff6600', 'MEDIUM': '#ff9900'}\n",
    "            color = color_map.get(alert['severity'], '#ff9900')\n",
    "            \n",
    "            # Create rich Slack message\n",
    "            message = {\n",
    "                \"attachments\": [\n",
    "                    {\n",
    "                        \"color\": color,\n",
    "                        \"title\": f\"🚨 K8s Cluster Disaster Alert - {alert['severity']} Priority\",\n",
    "                        \"text\": f\"Cluster resource exhaustion predicted for *{alert['metric']}*\",\n",
    "                        \"fields\": [\n",
    "                            {\n",
    "                                \"title\": \"Metric\",\n",
    "                                \"value\": alert['metric'],\n",
    "                                \"short\": True\n",
    "                            },\n",
    "                            {\n",
    "                                \"title\": \"Severity\",\n",
    "                                \"value\": alert['severity'],\n",
    "                                \"short\": True\n",
    "                            },\n",
    "                            {\n",
    "                                \"title\": \"Predicted Value at Breach\",\n",
    "                                \"value\": f\"{alert['predicted_value']:.1f}%\",\n",
    "                                \"short\": True\n",
    "                            },\n",
    "                            {\n",
    "                                \"title\": \"Maximum Predicted Value\",\n",
    "                                \"value\": f\"{alert['max_predicted_value']:.1f}%\",\n",
    "                                \"short\": True\n",
    "                            },\n",
    "                            {\n",
    "                                \"title\": \"Threshold\",\n",
    "                                \"value\": f\"{alert['threshold']:.0f}%\",\n",
    "                                \"short\": True\n",
    "                            },\n",
    "                            {\n",
    "                                \"title\": \"Time Until Breach\",\n",
    "                                \"value\": f\"{alert['time_until_breach_minutes']} minutes\",\n",
    "                                \"short\": True\n",
    "                            },\n",
    "                            {\n",
    "                                \"title\": \"Expected Breach Time\",\n",
    "                                \"value\": alert['breach_time'].strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                                \"short\": False\n",
    "                            },\n",
    "                            {\n",
    "                                \"title\": \"Duration Above Threshold\",\n",
    "                                \"value\": f\"{alert['affected_timespan_minutes']} minutes\",\n",
    "                                \"short\": True\n",
    "                            }\n",
    "                        ],\n",
    "                        \"footer\": \"K8s LSTM Disaster Recovery System\",\n",
    "                        \"ts\": int(datetime.now().timestamp())\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                response = requests.post(webhook_url, json=message)\n",
    "                response.raise_for_status()\n",
    "                print(f\"Alert sent for {alert['metric']} - {alert['severity']}\")\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Failed to send Slack alert: {e}\")\n",
    "    \n",
    "    def generate_prediction_plot(self, historical_data, predictions, current_time):\n",
    "        \"\"\"Generate visualization of predictions\"\"\"\n",
    "        plt.figure(figsize=(15, 8))\n",
    "        \n",
    "        # Create time arrays\n",
    "        historical_times = pd.date_range(\n",
    "            start=current_time - timedelta(minutes=len(historical_data)*5), \n",
    "            periods=len(historical_data), \n",
    "            freq='5T'\n",
    "        )\n",
    "        \n",
    "        prediction_times = pd.date_range(\n",
    "            start=current_time, \n",
    "            periods=len(predictions), \n",
    "            freq='5T'\n",
    "        )\n",
    "        \n",
    "        # Plot CPU\n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.plot(historical_times, historical_data[:, 0] * 100, 'b-', label='Historical CPU', linewidth=2)\n",
    "        plt.plot(prediction_times, predictions[:, 0] * 100, 'r--', label='Predicted CPU', linewidth=2)\n",
    "        plt.axhline(y=self.threshold * 100, color='orange', linestyle=':', label=f'Threshold ({self.threshold*100}%)')\n",
    "        plt.ylabel('CPU Usage (%)')\n",
    "        plt.title('Cluster CPU Usage Prediction')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot Memory\n",
    "        plt.subplot(2, 1, 2)\n",
    "        plt.plot(historical_times, historical_data[:, 1] * 100, 'b-', label='Historical Memory', linewidth=2)\n",
    "        plt.plot(prediction_times, predictions[:, 1] * 100, 'g--', label='Predicted Memory', linewidth=2)\n",
    "        plt.axhline(y=self.threshold * 100, color='orange', linestyle=':', label=f'Threshold ({self.threshold*100}%)')\n",
    "        plt.ylabel('Memory Usage (%)')\n",
    "        plt.xlabel('Time')\n",
    "        plt.title('Cluster Memory Usage Prediction')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5226868,
     "sourceId": 8712622,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
