{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.8/site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.8/site-packages (from seaborn) (1.24.4)\n",
      "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.8/site-packages (from seaborn) (2.0.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.8/site-packages (from seaborn) (3.7.5)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (6.4.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.1.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (25.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.1.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.57.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.8/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib!=3.6.1,>=3.4->seaborn) (3.20.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T08:52:24.521652Z",
     "iopub.status.busy": "2025-06-05T08:52:24.521295Z",
     "iopub.status.idle": "2025-06-05T08:52:31.818886Z",
     "shell.execute_reply": "2025-06-05T08:52:31.817898Z",
     "shell.execute_reply.started": "2025-06-05T08:52:24.521609Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.4.1\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "# Kubernetes LSTM Disaster Recovery System\n",
    "# Predicting CPU and Memory Usage for Pod-level Monitoring\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from datetime import datetime, timedelta\n",
    "import requests\n",
    "import json\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T03:01:57.935300Z",
     "iopub.status.busy": "2025-06-05T03:01:57.934956Z",
     "iopub.status.idle": "2025-06-05T03:01:57.952449Z",
     "shell.execute_reply": "2025-06-05T03:01:57.951461Z",
     "shell.execute_reply.started": "2025-06-05T03:01:57.935278Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 1. DATA LOADING AND PREPROCESSING\n",
    "# =============================================================================\n",
    "\n",
    "class DataProcessor:\n",
    "    def __init__(self, csv_path):\n",
    "        self.csv_path = csv_path\n",
    "        self.scaler_features = StandardScaler()\n",
    "        self.scaler_targets = MinMaxScaler()\n",
    "        \n",
    "    def load_and_preprocess(self):\n",
    "        \"\"\"Load CSV and perform initial preprocessing\"\"\"\n",
    "        print(\"Loading data from CSV...\")\n",
    "        df = pd.read_csv(self.csv_path)\n",
    "        \n",
    "        # Convert timestamp to datetime once, with your known format:\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'], format='%d/%m/%Y %H:%M')\n",
    "        \n",
    "        # Derive pod-level CPU and memory percentages\n",
    "        print(\"Deriving pod-level CPU and memory percentages...\")\n",
    "        df['pod_cpu_percentage'] = df['cpu_allocation_efficiency'] * df['node_cpu_usage']\n",
    "        df['pod_memory_percentage'] = df['memory_allocation_efficiency'] * df['node_memory_usage']\n",
    "        \n",
    "        # Clip values to 0–100%\n",
    "        df['pod_cpu_percentage']    = np.clip(df['pod_cpu_percentage'],    0, 100)\n",
    "        df['pod_memory_percentage'] = np.clip(df['pod_memory_percentage'], 0, 100)\n",
    "        \n",
    "        print(f\"Data shape: {df.shape}\")\n",
    "        print(f\"Unique pods: {df['pod_name'].nunique()}\")\n",
    "        print(f\"Date range: {df['timestamp'].min()} to {df['timestamp'].max()}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def resample_to_5min(self, df):\n",
    "        \"\"\"Resample 1-minute data to 5-minute intervals, per pod.\"\"\"\n",
    "        print(\"Resampling data from 1-minute to 5-minute intervals...\")\n",
    "        \n",
    "        numeric_cols = [\n",
    "            'cpu_allocation_efficiency', 'memory_allocation_efficiency',\n",
    "            'disk_io', 'network_latency', 'node_temperature',\n",
    "            'node_cpu_usage', 'node_memory_usage', 'pod_lifetime_seconds',\n",
    "            'pod_cpu_percentage', 'pod_memory_percentage'\n",
    "        ]\n",
    "        \n",
    "        resampled_dfs = []\n",
    "        for pod in df['pod_name'].unique():\n",
    "            pod_df = df[df['pod_name'] == pod].copy()\n",
    "            pod_df.set_index('timestamp', inplace=True)\n",
    "            \n",
    "            # Resample numeric features\n",
    "            r = pod_df[numeric_cols].resample('5T').mean()\n",
    "            # Grab the first event_type in each 5-min bin\n",
    "            r['event_type'] = pod_df['event_type'].resample('5T').first()\n",
    "            # Re-attach pod_name\n",
    "            r['pod_name'] = pod\n",
    "            \n",
    "            resampled_dfs.append(r)\n",
    "        \n",
    "        # Concatenate, drop any bins with missing data, and reset index\n",
    "        result = pd.concat(resampled_dfs)\n",
    "        result.dropna(inplace=True)\n",
    "        result = result.reset_index()  # brings timestamp back as column\n",
    "        \n",
    "        print(f\"Resampled data shape: {result.shape}\")\n",
    "        return result\n",
    "\n",
    "    def prepare_sequences(self, df, sequence_length=24, prediction_horizon=12):\n",
    "        \"\"\"\n",
    "        Per-pod sliding windows (will be commented out in the pipeline).\n",
    "        \"\"\"\n",
    "        feature_cols = [\n",
    "            'cpu_allocation_efficiency','memory_allocation_efficiency',\n",
    "            'disk_io','network_latency','node_temperature',\n",
    "            'node_cpu_usage','node_memory_usage','pod_lifetime_seconds'\n",
    "        ]\n",
    "        target_cols = ['pod_cpu_percentage','pod_memory_percentage']\n",
    "\n",
    "        sequences_X, sequences_y, pod_names, timestamps = [], [], [], []\n",
    "\n",
    "        for pod_name, grp in df.groupby('pod_name'):\n",
    "            grp = grp.sort_values('timestamp')\n",
    "            if len(grp) < sequence_length + prediction_horizon:\n",
    "                continue\n",
    "            feats = grp[feature_cols].values\n",
    "            targs = grp[target_cols].values\n",
    "\n",
    "            for i in range(len(grp) - sequence_length - prediction_horizon + 1):\n",
    "                sequences_X.append(feats[i:i+sequence_length])\n",
    "                sequences_y.append(targs[i+sequence_length:i+sequence_length+prediction_horizon])\n",
    "                pod_names.append(pod_name)\n",
    "                timestamps.append(grp['timestamp'].iloc[i+sequence_length])\n",
    "\n",
    "        return (\n",
    "            np.array(sequences_X),\n",
    "            np.array(sequences_y),\n",
    "            np.array(pod_names),\n",
    "            np.array(timestamps)\n",
    "        )\n",
    "\n",
    "    def prepare_sequences_all(self, df, sequence_length=24, prediction_horizon=12):\n",
    "        \"\"\"\n",
    "        Cluster-level sliding windows over the entire time series.\n",
    "        \"\"\"\n",
    "        feature_cols = [\n",
    "            'cpu_allocation_efficiency','memory_allocation_efficiency',\n",
    "            'disk_io','network_latency','node_temperature',\n",
    "            'node_cpu_usage','node_memory_usage','pod_lifetime_seconds'\n",
    "        ]\n",
    "        target_cols = ['pod_cpu_percentage','pod_memory_percentage']\n",
    "\n",
    "        df_sorted = df.sort_values('timestamp')\n",
    "        feats = df_sorted[feature_cols].values\n",
    "        targs = df_sorted[target_cols].values\n",
    "        times = df_sorted['timestamp'].values\n",
    "\n",
    "        Xs, Ys, Ts = [], [], []\n",
    "        max_i = len(df_sorted) - sequence_length - prediction_horizon + 1\n",
    "        for i in range(max_i):\n",
    "            Xs.append(feats[i : i + sequence_length])\n",
    "            Ys.append(targs[i + sequence_length : i + sequence_length + prediction_horizon])\n",
    "            Ts.append(times[i + sequence_length])\n",
    "\n",
    "        return np.array(Xs), np.array(Ys), np.array(Ts)\n",
    "    \n",
    "    def normalize_data(self, X_train, y_train, X_val=None, y_val=None):\n",
    "        \"\"\"Normalize features and targets\"\"\"\n",
    "        print(\"Normalizing data...\")\n",
    "        \n",
    "        # Reshape for scaling\n",
    "        X_train_reshaped = X_train.reshape(-1, X_train.shape[-1])\n",
    "        y_train_reshaped = y_train.reshape(-1, y_train.shape[-1])\n",
    "        \n",
    "        # Fit scalers on training data\n",
    "        X_train_scaled = self.scaler_features.fit_transform(X_train_reshaped)\n",
    "        y_train_scaled = self.scaler_targets.fit_transform(y_train_reshaped)\n",
    "        \n",
    "        # Reshape back\n",
    "        X_train_scaled = X_train_scaled.reshape(X_train.shape)\n",
    "        y_train_scaled = y_train_scaled.reshape(y_train.shape)\n",
    "        \n",
    "        if X_val is not None and y_val is not None:\n",
    "            X_val_reshaped = X_val.reshape(-1, X_val.shape[-1])\n",
    "            y_val_reshaped = y_val.reshape(-1, y_val.shape[-1])\n",
    "            \n",
    "            X_val_scaled = self.scaler_features.transform(X_val_reshaped)\n",
    "            y_val_scaled = self.scaler_targets.transform(y_val_reshaped)\n",
    "            \n",
    "            X_val_scaled = X_val_scaled.reshape(X_val.shape)\n",
    "            y_val_scaled = y_val_scaled.reshape(y_val.shape)\n",
    "            \n",
    "            return X_train_scaled, y_train_scaled, X_val_scaled, y_val_scaled\n",
    "        \n",
    "        print(\"Data Normalization Complete\")\n",
    "        return X_train_scaled, y_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T08:54:57.721840Z",
     "iopub.status.busy": "2025-06-05T08:54:57.721453Z",
     "iopub.status.idle": "2025-06-05T08:54:57.728206Z",
     "shell.execute_reply": "2025-06-05T08:54:57.726998Z",
     "shell.execute_reply.started": "2025-06-05T08:54:57.721815Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    " def load_and_preprocess(csv_path):\n",
    "    \"\"\"Load CSV and perform initial preprocessing\"\"\"\n",
    "    print(\"Loading data from CSV...\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Convert timestamp to datetime\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], format='%d/%m/%Y %H:%M')\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T08:55:55.502074Z",
     "iopub.status.busy": "2025-06-05T08:55:55.501017Z",
     "iopub.status.idle": "2025-06-05T08:55:55.575929Z",
     "shell.execute_reply": "2025-06-05T08:55:55.574752Z",
     "shell.execute_reply.started": "2025-06-05T08:55:55.502042Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/app/data/kaggle/input/kubernetes_performance_metrics_dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m csv_path \u001b[38;5;241m=\u001b[39m notebook_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkaggle\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkubernetes_performance_metrics_dataset.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/app/data/kaggle/input/kubernetes_performance_metrics_dataset.csv'"
     ]
    }
   ],
   "source": [
    "csv_path = \"./data/kaggle/input/kubernetes_performance_metrics_dataset.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T08:59:17.466680Z",
     "iopub.status.busy": "2025-06-05T08:59:17.466289Z",
     "iopub.status.idle": "2025-06-05T08:59:17.476680Z",
     "shell.execute_reply": "2025-06-05T08:59:17.475610Z",
     "shell.execute_reply.started": "2025-06-05T08:59:17.466655Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(\u001b[43mdf\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm/\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df['timestamp'] = pd.to_datetime(df['timestamp'], format='%d/%m/%Y %H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T08:59:44.994009Z",
     "iopub.status.busy": "2025-06-05T08:59:44.993594Z",
     "iopub.status.idle": "2025-06-05T08:59:45.015403Z",
     "shell.execute_reply": "2025-06-05T08:59:45.014112Z",
     "shell.execute_reply.started": "2025-06-05T08:59:44.993979Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>pod_name</th>\n",
       "      <th>namespace</th>\n",
       "      <th>cpu_allocation_efficiency</th>\n",
       "      <th>memory_allocation_efficiency</th>\n",
       "      <th>disk_io</th>\n",
       "      <th>network_latency</th>\n",
       "      <th>node_temperature</th>\n",
       "      <th>node_cpu_usage</th>\n",
       "      <th>node_memory_usage</th>\n",
       "      <th>event_type</th>\n",
       "      <th>event_message</th>\n",
       "      <th>scaling_event</th>\n",
       "      <th>pod_lifetime_seconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14995</th>\n",
       "      <td>2023-01-01 04:09:00</td>\n",
       "      <td>pod_14995</td>\n",
       "      <td>kube-system</td>\n",
       "      <td>0.020767</td>\n",
       "      <td>0.697208</td>\n",
       "      <td>379.511285</td>\n",
       "      <td>147.031290</td>\n",
       "      <td>66.820729</td>\n",
       "      <td>23.681710</td>\n",
       "      <td>65.269283</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Started</td>\n",
       "      <td>True</td>\n",
       "      <td>111871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14996</th>\n",
       "      <td>2023-01-01 04:09:00</td>\n",
       "      <td>pod_14996</td>\n",
       "      <td>default</td>\n",
       "      <td>0.026490</td>\n",
       "      <td>0.973705</td>\n",
       "      <td>93.014634</td>\n",
       "      <td>80.106794</td>\n",
       "      <td>21.358463</td>\n",
       "      <td>22.612871</td>\n",
       "      <td>48.674617</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Started</td>\n",
       "      <td>False</td>\n",
       "      <td>64848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14997</th>\n",
       "      <td>2023-01-01 04:09:00</td>\n",
       "      <td>pod_14997</td>\n",
       "      <td>dev</td>\n",
       "      <td>0.321295</td>\n",
       "      <td>0.073787</td>\n",
       "      <td>112.686558</td>\n",
       "      <td>83.621580</td>\n",
       "      <td>70.648406</td>\n",
       "      <td>74.516728</td>\n",
       "      <td>76.802551</td>\n",
       "      <td>Error</td>\n",
       "      <td>Failed</td>\n",
       "      <td>True</td>\n",
       "      <td>126843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14998</th>\n",
       "      <td>2023-01-01 04:09:00</td>\n",
       "      <td>pod_14998</td>\n",
       "      <td>kube-system</td>\n",
       "      <td>0.087156</td>\n",
       "      <td>0.322506</td>\n",
       "      <td>804.890194</td>\n",
       "      <td>158.398994</td>\n",
       "      <td>41.005118</td>\n",
       "      <td>58.788146</td>\n",
       "      <td>53.529527</td>\n",
       "      <td>Warning</td>\n",
       "      <td>Started</td>\n",
       "      <td>False</td>\n",
       "      <td>137157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14999</th>\n",
       "      <td>2023-01-01 04:09:00</td>\n",
       "      <td>pod_14999</td>\n",
       "      <td>default</td>\n",
       "      <td>0.094542</td>\n",
       "      <td>0.052845</td>\n",
       "      <td>624.286513</td>\n",
       "      <td>74.228015</td>\n",
       "      <td>21.776262</td>\n",
       "      <td>14.834679</td>\n",
       "      <td>32.077468</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Killed</td>\n",
       "      <td>True</td>\n",
       "      <td>112793</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                timestamp   pod_name    namespace  cpu_allocation_efficiency  \\\n",
       "14995 2023-01-01 04:09:00  pod_14995  kube-system                   0.020767   \n",
       "14996 2023-01-01 04:09:00  pod_14996      default                   0.026490   \n",
       "14997 2023-01-01 04:09:00  pod_14997          dev                   0.321295   \n",
       "14998 2023-01-01 04:09:00  pod_14998  kube-system                   0.087156   \n",
       "14999 2023-01-01 04:09:00  pod_14999      default                   0.094542   \n",
       "\n",
       "       memory_allocation_efficiency     disk_io  network_latency  \\\n",
       "14995                      0.697208  379.511285       147.031290   \n",
       "14996                      0.973705   93.014634        80.106794   \n",
       "14997                      0.073787  112.686558        83.621580   \n",
       "14998                      0.322506  804.890194       158.398994   \n",
       "14999                      0.052845  624.286513        74.228015   \n",
       "\n",
       "       node_temperature  node_cpu_usage  node_memory_usage event_type  \\\n",
       "14995         66.820729       23.681710          65.269283     Normal   \n",
       "14996         21.358463       22.612871          48.674617     Normal   \n",
       "14997         70.648406       74.516728          76.802551      Error   \n",
       "14998         41.005118       58.788146          53.529527    Warning   \n",
       "14999         21.776262       14.834679          32.077468     Normal   \n",
       "\n",
       "      event_message  scaling_event  pod_lifetime_seconds  \n",
       "14995       Started           True                111871  \n",
       "14996       Started          False                 64848  \n",
       "14997        Failed           True                126843  \n",
       "14998       Started          False                137157  \n",
       "14999        Killed           True                112793  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T09:00:32.621811Z",
     "iopub.status.busy": "2025-06-05T09:00:32.621408Z",
     "iopub.status.idle": "2025-06-05T09:00:32.631761Z",
     "shell.execute_reply": "2025-06-05T09:00:32.630759Z",
     "shell.execute_reply.started": "2025-06-05T09:00:32.621786Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"pod_name\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T03:01:58.221786Z",
     "iopub.status.busy": "2025-06-05T03:01:58.220962Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataProcessor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m csv_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/kaggle/input/kubernetes_performance_metrics_dataset.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m processor \u001b[38;5;241m=\u001b[39m \u001b[43mDataProcessor\u001b[49m(csv_path)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load and preprocess data\u001b[39;00m\n\u001b[1;32m      5\u001b[0m df \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mload_and_preprocess()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DataProcessor' is not defined"
     ]
    }
   ],
   "source": [
    "csv_path = \"./data/kaggle/input/kubernetes_performance_metrics_dataset.csv\"\n",
    "processor = DataProcessor(csv_path)\n",
    "\n",
    "# Load and preprocess data\n",
    "df = processor.load_and_preprocess()\n",
    "\n",
    "# Resample to 5-minute intervals\n",
    "df_resampled = processor.resample_to_5min(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_resampled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Prepare sequences\n",
    "# Option A: per-pod sequences (will likely give zero if each pod has only one timestamp)\n",
    "# X, y, pod_names, timestamps = processor.prepare_sequences(df_resampled)\n",
    "\n",
    "# Option B: cluster-level sequences (always works as long as len(df_resampled) ≥ seq_len + pred_horizon)\n",
    "X, y, timestamps = processor.prepare_sequences_all(df_resampled)\n",
    "\n",
    "# Train/validation split (80/20)\n",
    "split_idx = int(0.8 * len(X))\n",
    "\n",
    "X_train, X_val = X[:split_idx], X[split_idx:]\n",
    "y_train, y_val = y[:split_idx], y[split_idx:]\n",
    "\n",
    "# Normalize data\n",
    "X_train_scaled, y_train_scaled, X_val_scaled, y_val_scaled = processor.normalize_data(\n",
    "    X_train, y_train, X_val, y_val\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 2. LSTM MODEL ARCHITECTURE\n",
    "# =============================================================================\n",
    "\n",
    "class MultiOutputLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, prediction_horizon, dropout=0.2):\n",
    "        super(MultiOutputLSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.prediction_horizon = prediction_horizon\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Output layer that produces all future predictions at once\n",
    "        self.fc = nn.Linear(hidden_size, output_size * prediction_horizon)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                torch.nn.init.xavier_uniform_(param.data)\n",
    "            elif 'weight_hh' in name:\n",
    "                torch.nn.init.orthogonal_(param.data)\n",
    "            elif 'bias' in name:\n",
    "                param.data.fill_(0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Initialize hidden state\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "        \n",
    "        # LSTM forward pass\n",
    "        lstm_out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Use the last output\n",
    "        last_output = lstm_out[:, -1, :]\n",
    "        last_output = self.dropout(last_output)\n",
    "        \n",
    "        # Generate predictions\n",
    "        output = self.fc(last_output)\n",
    "        \n",
    "        # Reshape to (batch_size, prediction_horizon, output_size)\n",
    "        output = output.view(batch_size, self.prediction_horizon, self.output_size)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# =============================================================================\n",
    "# 3. TRAINING PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "class LSTMTrainer:\n",
    "    def __init__(self, model, device='cpu'):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.training_history = {'train_loss': [], 'val_loss': []}\n",
    "    \n",
    "    def train_model(self, X_train, y_train, X_val, y_val, epochs=100, batch_size=32, lr=0.001):\n",
    "        \"\"\"Train the LSTM model\"\"\"\n",
    "        print(f\"Training model on {self.device}...\")\n",
    "        \n",
    "        # Convert to tensors\n",
    "        X_train = torch.FloatTensor(X_train).to(self.device)\n",
    "        y_train = torch.FloatTensor(y_train).to(self.device)\n",
    "        X_val = torch.FloatTensor(X_val).to(self.device)\n",
    "        y_val = torch.FloatTensor(y_val).to(self.device)\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        # Optimizer and loss function\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.5)\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        patience = 15\n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Training\n",
    "            self.model.train()\n",
    "            train_loss = 0.0\n",
    "            \n",
    "            for batch_X, batch_y in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                predictions = self.model(batch_X)\n",
    "                loss = criterion(predictions, batch_y)\n",
    "                loss.backward()\n",
    "                \n",
    "                # Gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                \n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "            \n",
    "            # Validation\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_predictions = self.model(X_val)\n",
    "                val_loss = criterion(val_predictions, y_val).item()\n",
    "            \n",
    "            # Learning rate scheduling\n",
    "            scheduler.step(val_loss)\n",
    "            \n",
    "            # Track history\n",
    "            avg_train_loss = train_loss / len(train_loader)\n",
    "            self.training_history['train_loss'].append(avg_train_loss)\n",
    "            self.training_history['val_loss'].append(val_loss)\n",
    "            \n",
    "            # Early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                # Save best model\n",
    "                torch.save(self.model.state_dict(), 'best_model.pth')\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                    break\n",
    "            \n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {avg_train_loss:.6f}, Val Loss: {val_loss:.6f}')\n",
    "        \n",
    "        # Load best model\n",
    "        self.model.load_state_dict(torch.load('best_model.pth'))\n",
    "        print(\"Training completed!\")\n",
    "        \n",
    "        return self.training_history\n",
    "    \n",
    "    def plot_training_history(self):\n",
    "        \"\"\"Plot training and validation loss\"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(self.training_history['train_loss'], label='Training Loss')\n",
    "        plt.plot(self.training_history['val_loss'], label='Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training History')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 4. DISASTER PREDICTION AND ALERTING SYSTEM\n",
    "# =============================================================================\n",
    "\n",
    "class DisasterPredictor:\n",
    "    def __init__(self, model, scaler_features, scaler_targets, threshold=0.7, device='cpu'):\n",
    "        self.model = model.to(device)\n",
    "        self.scaler_features = scaler_features\n",
    "        self.scaler_targets = scaler_targets\n",
    "        self.threshold = threshold\n",
    "        self.device = device\n",
    "        self.model.eval()\n",
    "    \n",
    "    def predict_single_pod(self, pod_data):\n",
    "        \"\"\"\n",
    "        Predict future CPU/Memory usage for a single pod\n",
    "        pod_data: numpy array of shape (sequence_length, n_features)\n",
    "        \"\"\"\n",
    "        # Normalize input\n",
    "        pod_data_normalized = self.scaler_features.transform(pod_data)\n",
    "        \n",
    "        # Convert to tensor\n",
    "        X = torch.FloatTensor(pod_data_normalized).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            predictions = self.model(X)\n",
    "            predictions = predictions.cpu().numpy().squeeze()\n",
    "        \n",
    "        # Denormalize predictions\n",
    "        predictions_reshaped = predictions.reshape(-1, predictions.shape[-1])\n",
    "        predictions_denormalized = self.scaler_targets.inverse_transform(predictions_reshaped)\n",
    "        predictions_final = predictions_denormalized.reshape(predictions.shape)\n",
    "        \n",
    "        return predictions_final\n",
    "    \n",
    "    def check_disaster_conditions(self, predictions, pod_name, current_time):\n",
    "        \"\"\"\n",
    "        Check if predictions exceed disaster threshold\n",
    "        Returns alert information if disaster conditions are met\n",
    "        \"\"\"\n",
    "        cpu_predictions = predictions[:, 0]  # CPU percentage predictions\n",
    "        memory_predictions = predictions[:, 1]  # Memory percentage predictions\n",
    "        \n",
    "        # Convert to percentage (0-1 to 0-100 if needed)\n",
    "        if cpu_predictions.max() <= 1.0:\n",
    "            cpu_predictions *= 100\n",
    "            memory_predictions *= 100\n",
    "        \n",
    "        alerts = []\n",
    "        \n",
    "        # Check CPU threshold\n",
    "        cpu_breach_indices = np.where(cpu_predictions > self.threshold * 100)[0]\n",
    "        if len(cpu_breach_indices) > 0:\n",
    "            first_breach = cpu_breach_indices[0]\n",
    "            time_to_breach = first_breach * 5  # 5 minutes per prediction step\n",
    "            \n",
    "            alerts.append({\n",
    "                'pod_name': pod_name,\n",
    "                'metric': 'CPU',\n",
    "                'predicted_value': cpu_predictions[first_breach],\n",
    "                'threshold': self.threshold * 100,\n",
    "                'time_until_breach_minutes': time_to_breach,\n",
    "                'breach_time': current_time + timedelta(minutes=time_to_breach),\n",
    "                'severity': 'HIGH' if cpu_predictions[first_breach] > 90 else 'MEDIUM'\n",
    "            })\n",
    "        \n",
    "        # Check Memory threshold\n",
    "        memory_breach_indices = np.where(memory_predictions > self.threshold * 100)[0]\n",
    "        if len(memory_breach_indices) > 0:\n",
    "            first_breach = memory_breach_indices[0]\n",
    "            time_to_breach = first_breach * 5\n",
    "            \n",
    "            alerts.append({\n",
    "                'pod_name': pod_name,\n",
    "                'metric': 'Memory',\n",
    "                'predicted_value': memory_predictions[first_breach],\n",
    "                'threshold': self.threshold * 100,\n",
    "                'time_until_breach_minutes': time_to_breach,\n",
    "                'breach_time': current_time + timedelta(minutes=time_to_breach),\n",
    "                'severity': 'HIGH' if memory_predictions[first_breach] > 90 else 'MEDIUM'\n",
    "            })\n",
    "        \n",
    "        return alerts\n",
    "    \n",
    "    def send_slack_alert(self, alerts, webhook_url):\n",
    "        \"\"\"Send disaster alert to Slack\"\"\"\n",
    "        if not alerts:\n",
    "            return\n",
    "        \n",
    "        for alert in alerts:\n",
    "            color = '#ff0000' if alert['severity'] == 'HIGH' else '#ff9900'\n",
    "            \n",
    "            message = {\n",
    "                \"attachments\": [\n",
    "                    {\n",
    "                        \"color\": color,\n",
    "                        \"title\": f\"🚨 K8s Disaster Alert - {alert['severity']} Priority\",\n",
    "                        \"fields\": [\n",
    "                            {\n",
    "                                \"title\": \"Pod Name\",\n",
    "                                \"value\": alert['pod_name'],\n",
    "                                \"short\": True\n",
    "                            },\n",
    "                            {\n",
    "                                \"title\": \"Metric\",\n",
    "                                \"value\": alert['metric'],\n",
    "                                \"short\": True\n",
    "                            },\n",
    "                            {\n",
    "                                \"title\": \"Predicted Value\",\n",
    "                                \"value\": f\"{alert['predicted_value']:.1f}%\",\n",
    "                                \"short\": True\n",
    "                            },\n",
    "                            {\n",
    "                                \"title\": \"Threshold\",\n",
    "                                \"value\": f\"{alert['threshold']:.0f}%\",\n",
    "                                \"short\": True\n",
    "                            },\n",
    "                            {\n",
    "                                \"title\": \"Time Until Breach\",\n",
    "                                \"value\": f\"{alert['time_until_breach_minutes']} minutes\",\n",
    "                                \"short\": True\n",
    "                            },\n",
    "                            {\n",
    "                                \"title\": \"Expected Breach Time\",\n",
    "                                \"value\": alert['breach_time'].strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                                \"short\": True\n",
    "                            }\n",
    "                        ],\n",
    "                        \"footer\": \"K8s LSTM Disaster Recovery System\",\n",
    "                        \"ts\": int(datetime.now().timestamp())\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                response = requests.post(webhook_url, json=message)\n",
    "                response.raise_for_status()\n",
    "                print(f\"Alert sent for {alert['pod_name']} - {alert['metric']}\")\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Failed to send Slack alert: {e}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 5. MAIN EXECUTION PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "def main_pipeline(csv_path, slack_webhook_url=None):\n",
    "    \"\"\"Main execution pipeline for disaster recovery system\"\"\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"KUBERNETES LSTM DISASTER RECOVERY SYSTEM\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. Data Processing\n",
    "    processor = DataProcessor(csv_path)\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    df = processor.load_and_preprocess()\n",
    "    \n",
    "    # Resample to 5-minute intervals\n",
    "    df_resampled = processor.resample_to_5min(df)\n",
    "    \n",
    "    # Prepare sequences\n",
    "    X, y, pod_names, timestamps = processor.prepare_sequences(df_resampled)\n",
    "    \n",
    "    # Train/validation split (80/20)\n",
    "    split_idx = int(0.8 * len(X))\n",
    "    \n",
    "    X_train, X_val = X[:split_idx], X[split_idx:]\n",
    "    y_train, y_val = y[:split_idx], y[split_idx:]\n",
    "    \n",
    "    # Normalize data\n",
    "    X_train_scaled, y_train_scaled, X_val_scaled, y_val_scaled = processor.normalize_data(\n",
    "        X_train, y_train, X_val, y_val\n",
    "    )\n",
    "    \n",
    "    print(f\"Training samples: {len(X_train_scaled)}\")\n",
    "    print(f\"Validation samples: {len(X_val_scaled)}\")\n",
    "    \n",
    "    # 2. Model Training\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Model parameters\n",
    "    input_size = X_train_scaled.shape[2]  # Number of features\n",
    "    hidden_size = 64\n",
    "    num_layers = 2\n",
    "    output_size = 2  # CPU and Memory\n",
    "    prediction_horizon = 12  # 1 hour of 5-minute predictions\n",
    "    \n",
    "    # Initialize model\n",
    "    model = MultiOutputLSTM(\n",
    "        input_size=input_size,\n",
    "        hidden_size=hidden_size,\n",
    "        num_layers=num_layers,\n",
    "        output_size=output_size,\n",
    "        prediction_horizon=prediction_horizon,\n",
    "        dropout=0.3\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    trainer = LSTMTrainer(model, device)\n",
    "    history = trainer.train_model(\n",
    "        X_train_scaled, y_train_scaled, \n",
    "        X_val_scaled, y_val_scaled,\n",
    "        epochs=100, batch_size=32, lr=0.001\n",
    "    )\n",
    "    \n",
    "    # Plot training history\n",
    "    trainer.plot_training_history()\n",
    "    \n",
    "    # 3. Model Evaluation\n",
    "    print(\"\\n\" + \"=\" * 40)\n",
    "    print(\"MODEL EVALUATION\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Make predictions on validation set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_val_tensor = torch.FloatTensor(X_val_scaled).to(device)\n",
    "        val_predictions = model(X_val_tensor).cpu().numpy()\n",
    "    \n",
    "    # Denormalize predictions and targets\n",
    "    val_pred_reshaped = val_predictions.reshape(-1, val_predictions.shape[-1])\n",
    "    val_true_reshaped = y_val_scaled.reshape(-1, y_val_scaled.shape[-1])\n",
    "    \n",
    "    val_pred_denorm = processor.scaler_targets.inverse_transform(val_pred_reshaped)\n",
    "    val_true_denorm = processor.scaler_targets.inverse_transform(val_true_reshaped)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse_cpu = mean_squared_error(val_true_denorm[:, 0], val_pred_denorm[:, 0])\n",
    "    mae_cpu = mean_absolute_error(val_true_denorm[:, 0], val_pred_denorm[:, 0])\n",
    "    \n",
    "    mse_memory = mean_squared_error(val_true_denorm[:, 1], val_pred_denorm[:, 1])\n",
    "    mae_memory = mean_absolute_error(val_true_denorm[:, 1], val_pred_denorm[:, 1])\n",
    "    \n",
    "    print(f\"CPU Prediction - MSE: {mse_cpu:.4f}, MAE: {mae_cpu:.4f}\")\n",
    "    print(f\"Memory Prediction - MSE: {mse_memory:.4f}, MAE: {mae_memory:.4f}\")\n",
    "    \n",
    "    # 4. Disaster Prediction System\n",
    "    print(\"\\n\" + \"=\" * 40)\n",
    "    print(\"DISASTER PREDICTION SYSTEM\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    disaster_predictor = DisasterPredictor(\n",
    "        model=model,\n",
    "        scaler_features=processor.scaler_features,\n",
    "        scaler_targets=processor.scaler_targets,\n",
    "        threshold=0.7,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Test disaster prediction on a sample\n",
    "    if len(X_val) > 0:\n",
    "        sample_idx = 0\n",
    "        sample_pod = pod_names[split_idx + sample_idx]\n",
    "        sample_data = X_val[sample_idx]\n",
    "        \n",
    "        print(f\"Testing disaster prediction for pod: {sample_pod}\")\n",
    "        \n",
    "        # Make prediction\n",
    "        predictions = disaster_predictor.predict_single_pod(sample_data)\n",
    "        \n",
    "        # Check for disaster conditions\n",
    "        current_time = datetime.now()\n",
    "        alerts = disaster_predictor.check_disaster_conditions(\n",
    "            predictions, sample_pod, current_time\n",
    "        )\n",
    "        \n",
    "        if alerts:\n",
    "            print(f\"⚠️  DISASTER CONDITIONS DETECTED for {sample_pod}!\")\n",
    "            for alert in alerts:\n",
    "                print(f\"  - {alert['metric']}: {alert['predicted_value']:.1f}% \"\n",
    "                      f\"(threshold: {alert['threshold']:.0f}%) \"\n",
    "                      f\"in {alert['time_until_breach_minutes']} minutes\")\n",
    "            \n",
    "            # Send Slack alert if webhook provided\n",
    "            if slack_webhook_url:\n",
    "                disaster_predictor.send_slack_alert(alerts, slack_webhook_url)\n",
    "        else:\n",
    "            print(f\"✅ No disaster conditions detected for {sample_pod}\")\n",
    "    \n",
    "    # 5. Save Model and Components\n",
    "    print(\"\\n\" + \"=\" * 40)\n",
    "    print(\"SAVING MODEL COMPONENTS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), 'k8s_lstm_model.pth')\n",
    "    \n",
    "    # Save scalers\n",
    "    with open('feature_scaler.pkl', 'wb') as f:\n",
    "        pickle.dump(processor.scaler_features, f)\n",
    "    \n",
    "    with open('target_scaler.pkl', 'wb') as f:\n",
    "        pickle.dump(processor.scaler_targets, f)\n",
    "    \n",
    "    # Save model configuration\n",
    "    model_config = {\n",
    "        'input_size': input_size,\n",
    "        'hidden_size': hidden_size,\n",
    "        'num_layers': num_layers,\n",
    "        'output_size': output_size,\n",
    "        'prediction_horizon': prediction_horizon,\n",
    "        'threshold': 0.7\n",
    "    }\n",
    "    \n",
    "    with open('model_config.json', 'w') as f:\n",
    "        json.dump(model_config, f)\n",
    "    \n",
    "    print(\"Model and components saved successfully!\")\n",
    "    \n",
    "    return model, processor, disaster_predictor\n",
    "\n",
    "# =============================================================================\n",
    "# 6. DEPLOYMENT UTILITIES\n",
    "# =============================================================================\n",
    "\n",
    "def load_model_for_deployment(model_path='k8s_lstm_model.pth', \n",
    "                             config_path='model_config.json',\n",
    "                             feature_scaler_path='feature_scaler.pkl',\n",
    "                             target_scaler_path='target_scaler.pkl'):\n",
    "    \"\"\"Load trained model and components for deployment\"\"\"\n",
    "    \n",
    "    # Load configuration\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    # Load scalers\n",
    "    with open(feature_scaler_path, 'rb') as f:\n",
    "        feature_scaler = pickle.load(f)\n",
    "    \n",
    "    with open(target_scaler_path, 'rb') as f:\n",
    "        target_scaler = pickle.load(f)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = MultiOutputLSTM(\n",
    "        input_size=config['input_size'],\n",
    "        hidden_size=config['hidden_size'],\n",
    "        num_layers=config['num_layers'],\n",
    "        output_size=config['output_size'],\n",
    "        prediction_horizon=config['prediction_horizon']\n",
    "    )\n",
    "    \n",
    "    # Load trained weights\n",
    "    model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize disaster predictor\n",
    "    disaster_predictor = DisasterPredictor(\n",
    "        model=model,\n",
    "        scaler_features=feature_scaler,\n",
    "        scaler_targets=target_scaler,\n",
    "        threshold=config['threshold']\n",
    "    )\n",
    "    \n",
    "    return disaster_predictor\n",
    "\n",
    "def process_new_data_batch(disaster_predictor, csv_path, slack_webhook_url=None):\n",
    "    \"\"\"Process new CSV data for real-time monitoring\"\"\"\n",
    "    \n",
    "    print(f\"Processing new data batch: {csv_path}\")\n",
    "    \n",
    "    # Load new data\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    \n",
    "    # Derive pod percentages\n",
    "    df['pod_cpu_percentage'] = df['cpu_allocation_efficiency'] * df['node_cpu_usage']\n",
    "    df['pod_memory_percentage'] = df['memory_allocation_efficiency'] * df['node_memory_usage']\n",
    "    \n",
    "    # Feature columns\n",
    "    feature_cols = ['cpu_allocation_efficiency', 'memory_allocation_efficiency', \n",
    "                   'disk_io', 'network_latency', 'node_temperature', \n",
    "                   'node_cpu_usage', 'node_memory_usage', 'pod_lifesycle_seconds']\n",
    "    \n",
    "    all_alerts = []\n",
    "    \n",
    "    # Process each pod\n",
    "    for pod_name in df['pod_name'].unique():\n",
    "        pod_df = df[df['pod_name'] == pod_name].sort_values('timestamp')\n",
    "        \n",
    "        if len(pod_df) < 24:  # Need at least 24 time steps (2 hours)\n",
    "            continue\n",
    "        \n",
    "        # Get latest 24 data points\n",
    "        latest_data = pod_df[feature_cols].tail(24).values\n",
    "        \n",
    "        # Make prediction\n",
    "        try:\n",
    "            predictions = disaster_predictor.predict_single_pod(latest_data)\n",
    "            \n",
    "            # Check for alerts\n",
    "            current_time = datetime.now()\n",
    "            alerts = disaster_predictor.check_disaster_conditions(\n",
    "                predictions, pod_name, current_time\n",
    "            )\n",
    "            \n",
    "            all_alerts.extend(alerts)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing pod {pod_name}: {e}\")\n",
    "    \n",
    "    # Send alerts\n",
    "    if all_alerts and slack_webhook_url:\n",
    "        disaster_predictor.send_slack_alert(all_alerts, slack_webhook_url)\n",
    "        print(f\"Sent {len(all_alerts)} alerts to Slack\")\n",
    "    \n",
    "    return all_alerts\n",
    "\n",
    "# =============================================================================\n",
    "# 7. EXAMPLE USAGE AND TESTING\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 4. DISASTER PREDICTION AND ALERTING SYSTEM\n",
    "# =============================================================================\n",
    "\n",
    "class DisasterPredictor:\n",
    "    def __init__(self, model, scaler_features, scaler_targets, threshold=0.7, device='cpu'):\n",
    "        self.model = model.to(device)\n",
    "        self.scaler_features = scaler_features\n",
    "        self.scaler_targets = scaler_targets\n",
    "        self.threshold = threshold\n",
    "        self.device = device\n",
    "        self.model.eval()\n",
    "    \n",
    "    def predict_cluster_usage(self, cluster_data):\n",
    "        \"\"\"\n",
    "        Predict future CPU/Memory usage for the cluster\n",
    "        cluster_data: numpy array of shape (sequence_length, n_features)\n",
    "        \"\"\"\n",
    "        # Normalize input\n",
    "        cluster_data_normalized = self.scaler_features.transform(cluster_data)\n",
    "        \n",
    "        # Convert to tensor\n",
    "        X = torch.FloatTensor(cluster_data_normalized).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            predictions = self.model(X)\n",
    "            predictions = predictions.cpu().numpy().squeeze()\n",
    "        \n",
    "        # Denormalize predictions\n",
    "        predictions_reshaped = predictions.reshape(-1, predictions.shape[-1])\n",
    "        predictions_denormalized = self.scaler_targets.inverse_transform(predictions_reshaped)\n",
    "        predictions_final = predictions_denormalized.reshape(predictions.shape)\n",
    "        \n",
    "        return predictions_final\n",
    "    \n",
    "    def check_disaster_conditions(self, predictions, current_time):\n",
    "        \"\"\"\n",
    "        Check if predictions exceed disaster threshold for cluster\n",
    "        Returns alert information if disaster conditions are met\n",
    "        \"\"\"\n",
    "        cpu_predictions = predictions[:, 0]  # CPU percentage predictions\n",
    "        memory_predictions = predictions[:, 1]  # Memory percentage predictions\n",
    "        \n",
    "        # Convert to percentage (0-1 to 0-100 if needed)\n",
    "        if cpu_predictions.max() <= 1.0:\n",
    "            cpu_predictions *= 100\n",
    "            memory_predictions *= 100\n",
    "        \n",
    "        alerts = []\n",
    "        \n",
    "        # Check CPU threshold\n",
    "        cpu_breach_indices = np.where(cpu_predictions > self.threshold * 100)[0]\n",
    "        if len(cpu_breach_indices) > 0:\n",
    "            first_breach = cpu_breach_indices[0]\n",
    "            time_to_breach = first_breach * 5  # 5 minutes per prediction step\n",
    "            max_cpu = cpu_predictions[cpu_breach_indices].max()\n",
    "            \n",
    "            alerts.append({\n",
    "                'metric': 'Cluster CPU',\n",
    "                'predicted_value': cpu_predictions[first_breach],\n",
    "                'max_predicted_value': max_cpu,\n",
    "                'threshold': self.threshold * 100,\n",
    "                'time_until_breach_minutes': time_to_breach,\n",
    "                'breach_time': current_time + timedelta(minutes=time_to_breach),\n",
    "                'severity': 'CRITICAL' if max_cpu > 90 else 'HIGH' if max_cpu > 80 else 'MEDIUM',\n",
    "                'affected_timespan_minutes': len(cpu_breach_indices) * 5\n",
    "            })\n",
    "        \n",
    "        # Check Memory threshold\n",
    "        memory_breach_indices = np.where(memory_predictions > self.threshold * 100)[0]\n",
    "        if len(memory_breach_indices) > 0:\n",
    "            first_breach = memory_breach_indices[0]\n",
    "            time_to_breach = first_breach * 5\n",
    "            max_memory = memory_predictions[memory_breach_indices].max()\n",
    "            \n",
    "            alerts.append({\n",
    "                'metric': 'Cluster Memory',\n",
    "                'predicted_value': memory_predictions[first_breach],\n",
    "                'max_predicted_value': max_memory,\n",
    "                'threshold': self.threshold * 100,\n",
    "                'time_until_breach_minutes': time_to_breach,\n",
    "                'breach_time': current_time + timedelta(minutes=time_to_breach),\n",
    "                'severity': 'CRITICAL' if max_memory > 95 else 'HIGH' if max_memory > 85 else 'MEDIUM',\n",
    "                'affected_timespan_minutes': len(memory_breach_indices) * 5\n",
    "            })\n",
    "        \n",
    "        return alerts\n",
    "    \n",
    "    def send_slack_alert(self, alerts, webhook_url):\n",
    "        \"\"\"Send disaster alert to Slack\"\"\"\n",
    "        if not alerts:\n",
    "            return\n",
    "        \n",
    "        for alert in alerts:\n",
    "            # Color coding based on severity\n",
    "            color_map = {'CRITICAL': '#ff0000', 'HIGH': '#ff6600', 'MEDIUM': '#ff9900'}\n",
    "            color = color_map.get(alert['severity'], '#ff9900')\n",
    "            \n",
    "            # Create rich Slack message\n",
    "            message = {\n",
    "                \"attachments\": [\n",
    "                    {\n",
    "                        \"color\": color,\n",
    "                        \"title\": f\"🚨 K8s Cluster Disaster Alert - {alert['severity']} Priority\",\n",
    "                        \"text\": f\"Cluster resource exhaustion predicted for *{alert['metric']}*\",\n",
    "                        \"fields\": [\n",
    "                            {\n",
    "                                \"title\": \"Metric\",\n",
    "                                \"value\": alert['metric'],\n",
    "                                \"short\": True\n",
    "                            },\n",
    "                            {\n",
    "                                \"title\": \"Severity\",\n",
    "                                \"value\": alert['severity'],\n",
    "                                \"short\": True\n",
    "                            },\n",
    "                            {\n",
    "                                \"title\": \"Predicted Value at Breach\",\n",
    "                                \"value\": f\"{alert['predicted_value']:.1f}%\",\n",
    "                                \"short\": True\n",
    "                            },\n",
    "                            {\n",
    "                                \"title\": \"Maximum Predicted Value\",\n",
    "                                \"value\": f\"{alert['max_predicted_value']:.1f}%\",\n",
    "                                \"short\": True\n",
    "                            },\n",
    "                            {\n",
    "                                \"title\": \"Threshold\",\n",
    "                                \"value\": f\"{alert['threshold']:.0f}%\",\n",
    "                                \"short\": True\n",
    "                            },\n",
    "                            {\n",
    "                                \"title\": \"Time Until Breach\",\n",
    "                                \"value\": f\"{alert['time_until_breach_minutes']} minutes\",\n",
    "                                \"short\": True\n",
    "                            },\n",
    "                            {\n",
    "                                \"title\": \"Expected Breach Time\",\n",
    "                                \"value\": alert['breach_time'].strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                                \"short\": False\n",
    "                            },\n",
    "                            {\n",
    "                                \"title\": \"Duration Above Threshold\",\n",
    "                                \"value\": f\"{alert['affected_timespan_minutes']} minutes\",\n",
    "                                \"short\": True\n",
    "                            }\n",
    "                        ],\n",
    "                        \"footer\": \"K8s LSTM Disaster Recovery System\",\n",
    "                        \"ts\": int(datetime.now().timestamp())\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                response = requests.post(webhook_url, json=message)\n",
    "                response.raise_for_status()\n",
    "                print(f\"Alert sent for {alert['metric']} - {alert['severity']}\")\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Failed to send Slack alert: {e}\")\n",
    "    \n",
    "    def generate_prediction_plot(self, historical_data, predictions, current_time):\n",
    "        \"\"\"Generate visualization of predictions\"\"\"\n",
    "        plt.figure(figsize=(15, 8))\n",
    "        \n",
    "        # Create time arrays\n",
    "        historical_times = pd.date_range(\n",
    "            start=current_time - timedelta(minutes=len(historical_data)*5), \n",
    "            periods=len(historical_data), \n",
    "            freq='5T'\n",
    "        )\n",
    "        \n",
    "        prediction_times = pd.date_range(\n",
    "            start=current_time, \n",
    "            periods=len(predictions), \n",
    "            freq='5T'\n",
    "        )\n",
    "        \n",
    "        # Plot CPU\n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.plot(historical_times, historical_data[:, 0] * 100, 'b-', label='Historical CPU', linewidth=2)\n",
    "        plt.plot(prediction_times, predictions[:, 0] * 100, 'r--', label='Predicted CPU', linewidth=2)\n",
    "        plt.axhline(y=self.threshold * 100, color='orange', linestyle=':', label=f'Threshold ({self.threshold*100}%)')\n",
    "        plt.ylabel('CPU Usage (%)')\n",
    "        plt.title('Cluster CPU Usage Prediction')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot Memory\n",
    "        plt.subplot(2, 1, 2)\n",
    "        plt.plot(historical_times, historical_data[:, 1] * 100, 'b-', label='Historical Memory', linewidth=2)\n",
    "        plt.plot(prediction_times, predictions[:, 1] * 100, 'g--', label='Predicted Memory', linewidth=2)\n",
    "        plt.axhline(y=self.threshold * 100, color='orange', linestyle=':', label=f'Threshold ({self.threshold*100}%)')\n",
    "        plt.ylabel('Memory Usage (%)')\n",
    "        plt.xlabel('Time')\n",
    "        plt.title('Cluster Memory Usage Prediction')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5226868,
     "sourceId": 8712622,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
